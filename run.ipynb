{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "生产实例\n",
    "\n",
    "推荐安装： pip install pytrends fredapi yfinance\n",
    "使用许多实时公共数据源构建示例生产案例。\n",
    "\n",
    "虽然此处显示了股价预测，但单独的时间序列预测并不是管理投资的推荐基础！\n",
    "\n",
    "这是一种非常固执己见的方法。\n",
    "evolution = True 允许时间序列自动适应变化。\n",
    "\n",
    "然而，它存在陷入次优位置的轻微风险。\n",
    "它可能应该与一些基本的数据健全性检查相结合。\n",
    "\n",
    "cd ./AutoTS\n",
    "conda activate py38\n",
    "nohup python production_example.py > /dev/null &\n",
    "\"\"\"\n",
    "try:  # needs to go first\n",
    "    from sklearnex import patch_sklearn\n",
    "\n",
    "    patch_sklearn()\n",
    "except Exception as e:\n",
    "    print(repr(e))\n",
    "import json\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt  # required only for graphs \n",
    "from autots import AutoTS, load_live_daily, create_regressor\n",
    "\n",
    "fred_key = 'd84151f6309da8996e4f7627d6efc026'  # https://fred.stlouisfed.org/docs/api/api_key.html\n",
    "gsa_key = 'c3bd622a-44c4-472c-92f7-de6f2423634f' # https://open.gsa.gov/api/dap/\n",
    "\n",
    "forecast_name = \"example\"\n",
    "graph = True  # 是否绘制图形\n",
    "# https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects\n",
    "frequency = (\n",
    "    \"D\"  # “infer”用于自动对齐，但特定偏移量最可靠，“D”是每日\n",
    ")\n",
    "forecast_length = 60  #  未来预测的周期数\n",
    "drop_most_recent = 1  #  是否丢弃最近的n条记录（视为不完整）\n",
    "num_validations = (\n",
    "    2  # 交叉验证运行次数。 通常越多越好但速度越慢\n",
    ")\n",
    "validation_method = \"backwards\"  # \"similarity\", \"backwards\", \"seasonal 364\"\n",
    "n_jobs = \"auto\"  # 或设置为CPU核心数\n",
    "prediction_interval = (\n",
    "    0.9  # 通过概率范围设置预测范围的上限和下限。 更大=更宽 Bigger = wider\n",
    ")\n",
    "initial_training = \"auto\"  # 在第一次运行时将其设置为 True，或者在重置时，'auto' 会查找现有模板，如果找到，则设置为 False。\n",
    "evolve = True  # 允许时间序列在每次运行中逐步演化，如果为 False，则使用固定模板\n",
    "archive_templates = True  # 保存使用时间戳的模型模板的副本\n",
    "save_location = None  # \"C:/Users/Colin/Downloads\"  # 保存模板的目录。 默认为工作目录\n",
    "template_filename = f\"autots_forecast_template_{forecast_name}.csv\"\n",
    "forecast_csv_name = None  # f\"autots_forecast_{forecast_name}.csv\" 或 None，仅写入点预测\n",
    "model_list = \"scalable\"\n",
    "transformer_list = \"fast\"  # 'superfast'\n",
    "transformer_max_depth = 5\n",
    "models_mode = \"default\"  # \"deep\", \"regressor\"\n",
    "initial_template = 'random'  # 'random' 'general+random'\n",
    "preclean = None\n",
    "{  # preclean option\n",
    "    \"fillna\": 'ffill',\n",
    "    \"transformations\": {\"0\": \"EWMAFilter\"},\n",
    "    \"transformation_params\": {\n",
    "        \"0\": {\"span\": 14},\n",
    "    },\n",
    "}\n",
    "back_forecast = False\n",
    "csv_load = False\n",
    "start_time = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing template found.\n"
     ]
    }
   ],
   "source": [
    "if save_location is not None:\n",
    "    template_filename = os.path.join(save_location, template_filename)\n",
    "    if forecast_csv_name is not None:\n",
    "        forecast_csv_name = os.path.join(save_location, forecast_csv_name)\n",
    "\n",
    "if initial_training == \"auto\":\n",
    "    initial_training = not os.path.exists(template_filename)\n",
    "    if initial_training:\n",
    "        print(\"No existing template found.\")\n",
    "    else:\n",
    "        print(\"Existing template found.\")\n",
    "\n",
    "# 根据设置设置最大代数，增加速度会更慢，但获得最高准确度的机会更大\n",
    "# 如果在 import_templates 中指定了 include_ensemble，则集成可以逐步嵌套几代\n",
    "# if include_ensemble is specified in import_templates, ensembles can progressively nest over generations\n",
    "if initial_training:\n",
    "    gens = 100\n",
    "    generation_timeout = 10000  # minutes\n",
    "    models_to_validate = 0.15\n",
    "    ensemble = [\"horizontal-max\", \"dist\" \"simple\"]  #  \"mosaic\" \"mosaic-window\", 'mlensemble'\n",
    "elif evolve:\n",
    "    gens = 500\n",
    "    generation_timeout = 300  # minutes\n",
    "    models_to_validate = 0.15\n",
    "    ensemble = [\"horizontal-max\"]  # \"mosaic\", \"mosaic-window\", \"subsample\"\n",
    "else:\n",
    "    gens = 0\n",
    "    generation_timeout = 60  # minutes\n",
    "    models_to_validate = 0.99\n",
    "    ensemble = [\"horizontal-max\", \"dist\", \"simple\"]  # \"mosaic\", \"mosaic-window\",\n",
    "\n",
    "# 如果不进化，只保存最好的模型\n",
    "if evolve:\n",
    "    n_export = 50\n",
    "else:\n",
    "    n_export = 1  # > 1 不是一个坏主意，允许一些未来的适应性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "long: bool = False\n",
    "observation_start: str = None\n",
    "observation_end: str = None\n",
    "# fred_key: str = None\n",
    "fred_series=[\"DGS10\", \"T5YIE\", \"SP500\", \"DCOILWTICO\", \"DEXUSEU\", \"WPU0911\"]\n",
    "tickers: list = [\"MSFT\"]\n",
    "trends_list: list = [\"forecasting\", \"cycling\", \"microsoft\"]\n",
    "trends_geo: str = \"US\"\n",
    "weather_data_types: list = [\"AWND\", \"WSF2\", \"TAVG\"]\n",
    "weather_stations: list = [\"USW00013960\", \"USW00014925\"]\n",
    "weather_years: int = 6\n",
    "london_air_stations: list = ['CT3', 'SK8']\n",
    "london_air_species: str = \"PM25\"\n",
    "london_air_days: int = 700\n",
    "earthquake_days: int = 700\n",
    "earthquake_min_magnitude: int = 5\n",
    "# gsa_key: str = 'c3bd622a-44c4-472c-92f7-de6f2423634f'  # https://open.gsa.gov/api/dap/\n",
    "gov_domain_list=['nasa.gov']\n",
    "gov_domain_limit: int = 600\n",
    "wikipedia_pages: list = ['Microsoft_Office', \"List_of_highest-grossing_films\"]\n",
    "wiki_language: str = \"en\"\n",
    "weather_event_types=[\"%28Z%29+Winter+Weather\", \"%28Z%29+Winter+Storm\"]\n",
    "caiso_query: str = \"ENE_SLRS\"\n",
    "timeout: float = 300.05\n",
    "sleep_seconds: int = 15\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "earthquake_min_magnitude=5\n",
    "weather_years=3\n",
    "london_air_days=700\n",
    "gov_domain_list=None\n",
    "gov_domain_limit=700\n",
    "sleep_seconds=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Begin dataset retrieval 翻译：开始数据集检索\n",
    "\"\"\"\n",
    "import os\n",
    "# 设置代理\n",
    "os.environ['HTTP_PROXY'] = \"http://127.0.0.1:10809\"\n",
    "os.environ['HTTPS_PROXY'] = \"http://127.0.0.1:10809\"\n",
    "\n",
    "if not csv_load:\n",
    "    fred_series = [\n",
    "        \"DGS10\",\n",
    "        \"T5YIE\",\n",
    "        \"SP500\",\n",
    "        \"DCOILWTICO\",\n",
    "        \"DEXUSUK\",\n",
    "        \"DEXUSEU\",\n",
    "        \"BAMLH0A0HYM2\",\n",
    "        \"DAAA\",\n",
    "        \"T10Y2Y\",\n",
    "    ]\n",
    "    tickers = [\"MSFT\", \"PG\"] # \n",
    "    trend_list = [\"forecasting\", \"msft\", \"p&g\"]\n",
    "    weather_event_types = [\"%28Z%29+Winter+Weather\", \"%28Z%29+Winter+Storm\"]\n",
    "    wikipedia_pages = ['all', 'Microsoft', \"Procter_%26_Gamble\", \"YouTube\", \"United_States\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下载数据，跳过"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sleep_seconds >= 0.5, \"sleep_seconds must be >=0.5\"\n",
    "\n",
    "dataset_lists = []\n",
    "if observation_end is None:\n",
    "    current_date = datetime.datetime.utcnow()\n",
    "else:\n",
    "    current_date = observation_end\n",
    "if observation_start is None:\n",
    "    # should take from observation_end but that's expected as a string\n",
    "    observation_start = datetime.datetime.utcnow() - datetime.timedelta(\n",
    "        days=365 * 6\n",
    "    )\n",
    "    observation_start = observation_start.strftime(\"%Y-%m-%d\")\n",
    "try:\n",
    "    import requests\n",
    "\n",
    "    s = requests.Session()\n",
    "except Exception as e:\n",
    "    print(f\"requests Session creation failed {repr(e)}\")\n",
    "\n",
    "if fred_key is not None and fred_series is not None:\n",
    "    from autots.datasets.fred2 import Fred  # noqa\n",
    "    from autots.datasets.fred import get_fred_data\n",
    "\n",
    "    fred_df = get_fred_data(\n",
    "        fred_key,\n",
    "        fred_series,\n",
    "        long=False,\n",
    "        observation_start=observation_start,\n",
    "        sleep_seconds=sleep_seconds,\n",
    "    )\n",
    "    # fred_df.index = fred_df.index.tz_localize(None)\n",
    "    # dataset_lists.append(fred_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义current_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_lists = []\n",
    "if observation_end is None:\n",
    "    current_date = datetime.datetime.utcnow()\n",
    "else:\n",
    "    current_date = observation_end\n",
    "if observation_start is None:\n",
    "    # should take from observation_end but that's expected as a string\n",
    "    observation_start = datetime.datetime.utcnow() - datetime.timedelta(\n",
    "        days=365 * 6\n",
    "    )\n",
    "    observation_start = observation_start.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "try:\n",
    "    import requests\n",
    "\n",
    "    s = requests.Session()\n",
    "except Exception as e:\n",
    "    print(f\"requests Session creation failed {repr(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 从fred_data.csv 读取数据到dataset_lists\n",
    "# fred_df = pd.read_csv('fred_data.csv', index_col=0)\n",
    "import pickle\n",
    "with open('dataset_lists.pkl', 'rb') as f:\n",
    "    dataset_lists = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            DGS10  T5YIE    SP500  DCOILWTICO  DEXUSUK  DEXUSEU  BAMLH0A0HYM2  \\\n",
      "2018-01-25   2.63   1.91  2839.25       65.62   1.4264   1.2488          3.28   \n",
      "2018-01-26   2.66   1.93  2872.87       66.27   1.4179   1.2422          3.23   \n",
      "2018-01-29   2.70   1.92  2853.53       65.71   1.4042   1.2352          3.26   \n",
      "2018-01-30   2.73   1.97  2822.43       64.64   1.4124   1.2390          3.33   \n",
      "2018-01-31   2.72   1.98  2823.81       64.82   1.4190   1.2428          3.29   \n",
      "\n",
      "            DAAA  T10Y2Y  \n",
      "2018-01-25  3.55    0.55  \n",
      "2018-01-26  3.58    0.53  \n",
      "2018-01-29  3.59    0.59  \n",
      "2018-01-30  3.63    0.60  \n",
      "2018-01-31  3.59    0.58  \n",
      "(1584, 9)\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# dataset_lists = []\n",
    "# dataset_lists.append(fred_df) # 加入数据集列表\n",
    "# 输出list列表前五行\n",
    "print(dataset_lists[0].head())\n",
    "# 打印数据集的形状\n",
    "print(dataset_lists[0].shape)\n",
    "# 打印数据集的数量\n",
    "print(len(dataset_lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 series downloaded.\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "# 首先确保所有数据集的索引都转换为统一的日期时间格式\n",
    "dataset_lists = [dataset.set_index(pd.to_datetime(dataset.index)) for dataset in dataset_lists]\n",
    "\n",
    "\n",
    "df = reduce(\n",
    "    lambda x, y: pd.merge(x, y, left_index=True, right_index=True, how=\"outer\"), # 合并数据集\n",
    "    dataset_lists,\n",
    ")\n",
    "print(f\"{df.shape[1]} series downloaded.\")\n",
    "s.close()\n",
    "df.index.name = \"datetime\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long = df.reset_index(drop=False).melt(\n",
    "    id_vars=['datetime'], var_name='series_id', value_name='value'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tickers is not None:\n",
    "    for fx in tickers:\n",
    "        for suffix in [\"_high\", \"_low\", \"_open\", \"_close\"]:\n",
    "            fxs = (fx + suffix).lower()\n",
    "            if fxs in df.columns:\n",
    "                df[fxs] = df[fxs].interpolate('akima')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fred_series is not None:\n",
    "    for fx in fred_series:\n",
    "        if fx in df.columns:\n",
    "            df[fx] = df[fx].interpolate('akima')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.ffill(limit=3) # 填充缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.index <= start_time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[x for x in df.columns if \"_volume\" not in x]]\n",
    "# 取消股息和股票分割，因为它会扭曲指标\n",
    "df = df[[x for x in df.columns if \"_dividends\" not in x]]\n",
    "df = df[[x for x in df.columns if \"stock_splits\" not in x]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"training_data_{forecast_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency: str = \"infer\"\n",
    "holiday_countries: list = [\"CN\"]\n",
    "datepart_method: str = \"simple_binarized\"\n",
    "drop_most_recent: int = 0\n",
    "scale: bool = True\n",
    "summarize: str = \"auto\"\n",
    "backfill: str = \"bfill\"\n",
    "n_jobs: str = \"auto\"\n",
    "fill_na: str = 'ffill'\n",
    "aggfunc: str = \"first\"\n",
    "encode_holiday_type=False\n",
    "holiday_detector_params={\n",
    "    \"threshold\": 0.8,\n",
    "    \"splash_threshold\": None, # 设定一个界限，用于区分显著的假日效应\n",
    "    \"use_dayofmonth_holidays\": True, # 月份中固定日期的假日\n",
    "    \"use_wkdom_holidays\": True, # 月初的工作日假期\n",
    "    \"use_wkdeom_holidays\": False, # 月底的工作日假期\n",
    "    \"use_lunar_holidays\": True, # 农历假期，如春节\n",
    "    \"use_lunar_weekday\": False, # 农历的工作日\n",
    "    \"use_islamic_holidays\": False, # 伊斯兰假期\n",
    "    \"use_hebrew_holidays\": False, # 希伯来假期（例如，犹太新年）\n",
    "    \"output\": 'univariate',\n",
    "    \"anomaly_detector_params\": { # 异常检测器参数\n",
    "        \"method\": \"mad\", # 异常检测方法，例如\"mad\"代表中位数绝对偏差\n",
    "        \"transform_dict\": {\n",
    "            \"fillna\": None,\n",
    "            \"transformations\": {\"0\": \"DifferencedTransformer\"},\n",
    "            \"transformation_params\": {\"0\": {}},\n",
    "        },\n",
    "        \"forecast_params\": None,\n",
    "        \"method_params\": {\"distribution\": \"gamma\", \"alpha\": 0.05},\n",
    "    },\n",
    "},\n",
    "holiday_regr_style: str = \"flag\"\n",
    "preprocessing_params: dict = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autots.tools.shaping import infer_frequency\n",
    "if frequency == \"infer\": # 从数据中推断频率\n",
    "    frequency = infer_frequency(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.resample(frequency).first() # 重采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns: #尝试将所有列转换为数值类型\n",
    "    try:\n",
    "        df[col] = pd.to_numeric(df[col])\n",
    "    except ValueError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select_dtypes(include=np.number)  # 选择数值类型的列\n",
    "dates = df.index # 获取索引\n",
    "df_cols = df.columns\n",
    "df_inner = df.copy()\n",
    "\n",
    "if scale: # 如果为真，使用 StandardScaler 标准化特征\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    df_inner = pd.DataFrame(\n",
    "        scaler.fit_transform(df_inner), index=dates, columns=df_cols\n",
    "    )\n",
    "\n",
    "\n",
    "from autots.tools.impute import FillNA\n",
    " # 填充假期的空数据（前面已新建假期索引日期），然后数据降维方法选择\n",
    "df_inner = FillNA(df_inner, method=fill_na)\n",
    "ag_flag = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_inner 压缩维度到15维\n",
    "from sklearn.cluster import FeatureAgglomeration\n",
    "\n",
    "n_clusters = 10 if ag_flag else 15\n",
    "if df_inner.shape[1] > 15:\n",
    "    df_inner = pd.DataFrame(\n",
    "        FeatureAgglomeration(n_clusters=n_clusters).fit_transform(df_inner),\n",
    "        index=dates,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tail() 返回最后n行\n",
    "regressor_forecast = df_inner.tail(forecast_length)\n",
    "\n",
    "# 重置索引，将开始日期设置为数据最后一天，依次往后延申\n",
    "regressor_forecast.index = pd.date_range(\n",
    "    dates[-1], periods=(forecast_length + 1), freq=frequency\n",
    ")[1:]\n",
    "\n",
    "# 数据时间索引整体推迟60天\n",
    "regressor_train = df_inner.shift(forecast_length)\n",
    "\n",
    "# 通过先向后填充再向前填充的方式来处理regressor_train中的任何缺失值\n",
    "regressor_train = regressor_train.bfill().ffill()\n",
    "\n",
    "regr_train = regressor_train.copy()\n",
    "regr_fcst = regressor_forecast.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autots.tools.seasonal import date_part\n",
    "# datepart\n",
    "if datepart_method is not None:\n",
    "    regr_train = pd.concat(\n",
    "        [regr_train, date_part(regr_train.index, method=datepart_method)],\n",
    "        axis=1,\n",
    "    )\n",
    "    regr_fcst = pd.concat(\n",
    "        [regr_fcst, date_part(regr_fcst.index, method=datepart_method)],\n",
    "        axis=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[forecast_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_train = regr_train.iloc[forecast_length:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autots",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
