{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "生产实例\n",
    "\n",
    "推荐安装： pip install pytrends fredapi yfinance\n",
    "使用许多实时公共数据源构建示例生产案例。\n",
    "\n",
    "虽然此处显示了股价预测，但单独的时间序列预测并不是管理投资的推荐基础！\n",
    "\n",
    "这是一种非常固执己见的方法。\n",
    "evolution = True 允许时间序列自动适应变化。\n",
    "\n",
    "然而，它存在陷入次优位置的轻微风险。\n",
    "它可能应该与一些基本的数据健全性检查相结合。\n",
    "\n",
    "cd ./AutoTS\n",
    "conda activate py38\n",
    "nohup python production_example.py > /dev/null &\n",
    "\"\"\"\n",
    "try:  # needs to go first\n",
    "    from sklearnex import patch_sklearn\n",
    "\n",
    "    patch_sklearn()\n",
    "except Exception as e:\n",
    "    print(repr(e))\n",
    "import json\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt  # required only for graphs \n",
    "from autots import AutoTS, load_live_daily, create_regressor\n",
    "\n",
    "fred_key = 'd84151f6309da8996e4f7627d6efc026'  # https://fred.stlouisfed.org/docs/api/api_key.html\n",
    "gsa_key = 'c3bd622a-44c4-472c-92f7-de6f2423634f' # https://open.gsa.gov/api/dap/\n",
    "\n",
    "forecast_name = \"example\"\n",
    "graph = True  # 是否绘制图形\n",
    "# https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects\n",
    "frequency = (\n",
    "    \"D\"  # “infer”用于自动对齐，但特定偏移量最可靠，“D”是每日\n",
    ")\n",
    "forecast_length = 60  #  未来预测的周期数\n",
    "drop_most_recent = 1  #  是否丢弃最近的n条记录（视为不完整）\n",
    "num_validations = (\n",
    "    2  # 交叉验证运行次数。 通常越多越好但速度越慢\n",
    ")\n",
    "validation_method = \"backwards\"  # \"similarity\", \"backwards\", \"seasonal 364\"\n",
    "n_jobs = \"auto\"  # 或设置为CPU核心数\n",
    "prediction_interval = (\n",
    "    0.9  # 通过概率范围设置预测范围的上限和下限。 更大=更宽 Bigger = wider\n",
    ")\n",
    "initial_training = \"auto\"  # 在第一次运行时将其设置为 True，或者在重置时，'auto' 会查找现有模板，如果找到，则设置为 False。\n",
    "evolve = True  # 允许时间序列在每次运行中逐步演化，如果为 False，则使用固定模板\n",
    "archive_templates = True  # 保存使用时间戳的模型模板的副本\n",
    "save_location = None  # \"C:/Users/Colin/Downloads\"  # 保存模板的目录。 默认为工作目录\n",
    "template_filename = f\"autots_forecast_template_{forecast_name}.csv\"\n",
    "forecast_csv_name = None  # f\"autots_forecast_{forecast_name}.csv\" 或 None，仅写入点预测\n",
    "model_list = \"scalable\"\n",
    "transformer_list = \"fast\"  # 'superfast'\n",
    "transformer_max_depth = 5\n",
    "models_mode = \"default\"  # \"deep\", \"regressor\"\n",
    "initial_template = 'random'  # 'random' 'general+random'\n",
    "preclean = None\n",
    "{  # preclean option\n",
    "    \"fillna\": 'ffill',\n",
    "    \"transformations\": {\"0\": \"EWMAFilter\"},\n",
    "    \"transformation_params\": {\n",
    "        \"0\": {\"span\": 14},\n",
    "    },\n",
    "}\n",
    "back_forecast = False\n",
    "csv_load = False\n",
    "start_time = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing template found.\n"
     ]
    }
   ],
   "source": [
    "if save_location is not None:\n",
    "    template_filename = os.path.join(save_location, template_filename)\n",
    "    if forecast_csv_name is not None:\n",
    "        forecast_csv_name = os.path.join(save_location, forecast_csv_name)\n",
    "\n",
    "if initial_training == \"auto\":\n",
    "    initial_training = not os.path.exists(template_filename)\n",
    "    if initial_training:\n",
    "        print(\"No existing template found.\")\n",
    "    else:\n",
    "        print(\"Existing template found.\")\n",
    "\n",
    "# 根据设置设置最大代数，增加速度会更慢，但获得最高准确度的机会更大\n",
    "# 如果在 import_templates 中指定了 include_ensemble，则集成可以逐步嵌套几代\n",
    "# if include_ensemble is specified in import_templates, ensembles can progressively nest over generations\n",
    "if initial_training:\n",
    "    gens = 100\n",
    "    generation_timeout = 10000  # minutes\n",
    "    models_to_validate = 0.15\n",
    "    ensemble = [\"horizontal-max\", \"dist\", \"simple\"]  # , \"mosaic\", \"mosaic-window\", 'mlensemble'\n",
    "elif evolve:\n",
    "    gens = 500\n",
    "    generation_timeout = 300  # minutes\n",
    "    models_to_validate = 0.15\n",
    "    ensemble = [\"horizontal-max\"]  # \"mosaic\", \"mosaic-window\", \"subsample\"\n",
    "else:\n",
    "    gens = 0\n",
    "    generation_timeout = 60  # minutes\n",
    "    models_to_validate = 0.99\n",
    "    ensemble = [\"horizontal-max\", \"dist\", \"simple\"]  # \"mosaic\", \"mosaic-window\",\n",
    "\n",
    "# 如果不进化，只保存最好的模型\n",
    "if evolve:\n",
    "    n_export = 50\n",
    "else:\n",
    "    n_export = 1  # > 1 不是一个坏主意，允许一些未来的适应性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# long: bool = False\n",
    "observation_start: str = None\n",
    "observation_end: str = None\n",
    "# fred_key: str = None\n",
    "fred_series=[\"DGS10\", \"T5YIE\", \"SP500\", \"DCOILWTICO\", \"DEXUSEU\", \"WPU0911\"]\n",
    "tickers: list = [\"MSFT\"]\n",
    "trends_list: list = [\"forecasting\", \"cycling\", \"microsoft\"]\n",
    "trends_geo: str = \"US\"\n",
    "weather_data_types: list = [\"AWND\", \"WSF2\", \"TAVG\"]\n",
    "weather_stations: list = [\"USW00013960\", \"USW00014925\"]\n",
    "weather_years: int = 6\n",
    "london_air_stations: list = ['CT3', 'SK8']\n",
    "london_air_species: str = \"PM25\"\n",
    "london_air_days: int = 700\n",
    "earthquake_days: int = 700\n",
    "earthquake_min_magnitude: int = 5\n",
    "# gsa_key: str = 'c3bd622a-44c4-472c-92f7-de6f2423634f'  # https://open.gsa.gov/api/dap/\n",
    "gov_domain_list=['nasa.gov']\n",
    "gov_domain_limit: int = 600\n",
    "wikipedia_pages: list = ['Microsoft_Office', \"List_of_highest-grossing_films\"]\n",
    "wiki_language: str = \"en\"\n",
    "weather_event_types=[\"%28Z%29+Winter+Weather\", \"%28Z%29+Winter+Storm\"]\n",
    "caiso_query: str = \"ENE_SLRS\"\n",
    "timeout: float = 300.05\n",
    "sleep_seconds: int = 15\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "earthquake_min_magnitude=5\n",
    "weather_years=3\n",
    "london_air_days=700\n",
    "gov_domain_list=None\n",
    "gov_domain_limit=700\n",
    "sleep_seconds=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Begin dataset retrieval 翻译：开始数据集检索\n",
    "\"\"\"\n",
    "import os\n",
    "# 设置代理\n",
    "os.environ['HTTP_PROXY'] = \"http://127.0.0.1:10809\"\n",
    "os.environ['HTTPS_PROXY'] = \"http://127.0.0.1:10809\"\n",
    "\n",
    "if not csv_load:\n",
    "    fred_series = [\n",
    "        \"DGS10\",\n",
    "        \"T5YIE\",\n",
    "        \"SP500\",\n",
    "        \"DCOILWTICO\",\n",
    "        \"DEXUSUK\",\n",
    "        \"DEXUSEU\",\n",
    "        \"BAMLH0A0HYM2\",\n",
    "        \"DAAA\",\n",
    "        \"T10Y2Y\",\n",
    "    ]\n",
    "    tickers = [\"MSFT\", \"PG\"] # \n",
    "    trend_list = [\"forecasting\", \"msft\", \"p&g\"]\n",
    "    weather_event_types = [\"%28Z%29+Winter+Weather\", \"%28Z%29+Winter+Storm\"]\n",
    "    wikipedia_pages = ['all', 'Microsoft', \"Procter_%26_Gamble\", \"YouTube\", \"United_States\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下载数据，跳过"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在下载 DGS10...\n"
     ]
    },
    {
     "ename": "URLError",
     "evalue": "<urlopen error [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSSLEOFError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32md:\\miniconda3\\envs\\ts2\\lib\\urllib\\request.py:1348\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1347\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1348\u001b[0m     \u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1349\u001b[0m \u001b[43m              \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhas_header\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTransfer-encoding\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1350\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\ts2\\lib\\http\\client.py:1283\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1283\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\ts2\\lib\\http\\client.py:1329\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1328\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1329\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\ts2\\lib\\http\\client.py:1278\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1277\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[1;32m-> 1278\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\ts2\\lib\\http\\client.py:1038\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[1;32m-> 1038\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1041\u001b[0m \n\u001b[0;32m   1042\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\ts2\\lib\\http\\client.py:976\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[1;32m--> 976\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\ts2\\lib\\http\\client.py:1455\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1453\u001b[0m     server_hostname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n\u001b[1;32m-> 1455\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1456\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\ts2\\lib\\ssl.py:513\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    508\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    509\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    510\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    511\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[1;32m--> 513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msslsocket_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\ts2\\lib\\ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1103\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1104\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\ts2\\lib\\ssl.py:1375\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1374\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1375\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mSSLEOFError\u001b[0m: [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mautots\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfred2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Fred  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mautots\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfred\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_fred_data\n\u001b[1;32m---> 25\u001b[0m fred_df \u001b[38;5;241m=\u001b[39m \u001b[43mget_fred_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfred_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfred_series\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlong\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobservation_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobservation_start\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_seconds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msleep_seconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# fred_df.index = fred_df.index.tz_localize(None)\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# dataset_lists.append(fred_df)\u001b[39;00m\n",
      "File \u001b[1;32md:\\pytorch\\AutoTS\\autots\\datasets\\fred.py:72\u001b[0m, in \u001b[0;36mget_fred_data\u001b[1;34m(fredkey, SeriesNameDict, long, observation_start, sleep_seconds, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m series \u001b[38;5;129;01min\u001b[39;00m series_desired:\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m正在下载 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseries\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 72\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mfred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservation_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobservation_start\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m         series_name \u001b[38;5;241m=\u001b[39m SeriesNameDict[series]\n",
      "File \u001b[1;32md:\\pytorch\\AutoTS\\autots\\datasets\\fred2.py:131\u001b[0m, in \u001b[0;36mFred.get_series\u001b[1;34m(self, series_id, observation_start, observation_end, **kwargs)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    130\u001b[0m     url \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m urlencode(kwargs)\n\u001b[1;32m--> 131\u001b[0m root \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__fetch_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m root \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo data exists for series id: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m series_id)\n",
      "File \u001b[1;32md:\\pytorch\\AutoTS\\autots\\datasets\\fred2.py:64\u001b[0m, in \u001b[0;36mFred.__fetch_data\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m     62\u001b[0m url \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m&api_key=\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m     root \u001b[38;5;241m=\u001b[39m ET\u001b[38;5;241m.\u001b[39mfromstring(response\u001b[38;5;241m.\u001b[39mread())\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\ts2\\lib\\urllib\\request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\ts2\\lib\\urllib\\request.py:519\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    516\u001b[0m     req \u001b[38;5;241m=\u001b[39m meth(req)\n\u001b[0;32m    518\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n\u001b[1;32m--> 519\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[0;32m    522\u001b[0m meth_name \u001b[38;5;241m=\u001b[39m protocol\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_response\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\ts2\\lib\\urllib\\request.py:536\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[1;34m(self, req, data)\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m    535\u001b[0m protocol \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n\u001b[1;32m--> 536\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[0;32m    537\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_open\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[0;32m    539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\ts2\\lib\\urllib\\request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\ts2\\lib\\urllib\\request.py:1391\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[1;34m(self, req)\u001b[0m\n\u001b[0;32m   1390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[1;32m-> 1391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHTTPSConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\ts2\\lib\\urllib\\request.py:1351\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1348\u001b[0m         h\u001b[38;5;241m.\u001b[39mrequest(req\u001b[38;5;241m.\u001b[39mget_method(), req\u001b[38;5;241m.\u001b[39mselector, req\u001b[38;5;241m.\u001b[39mdata, headers,\n\u001b[0;32m   1349\u001b[0m                   encode_chunked\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39mhas_header(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransfer-encoding\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m   1350\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[1;32m-> 1351\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n\u001b[0;32m   1352\u001b[0m     r \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m   1353\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[1;31mURLError\u001b[0m: <urlopen error [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)>"
     ]
    }
   ],
   "source": [
    "assert sleep_seconds >= 0.5, \"sleep_seconds must be >=0.5\"\n",
    "\n",
    "dataset_lists = []\n",
    "if observation_end is None:\n",
    "    current_date = datetime.datetime.utcnow()\n",
    "else:\n",
    "    current_date = observation_end\n",
    "if observation_start is None:\n",
    "    # should take from observation_end but that's expected as a string\n",
    "    observation_start = datetime.datetime.utcnow() - datetime.timedelta(\n",
    "        days=365 * 6\n",
    "    )\n",
    "    observation_start = observation_start.strftime(\"%Y-%m-%d\")\n",
    "try:\n",
    "    import requests\n",
    "\n",
    "    s = requests.Session()\n",
    "except Exception as e:\n",
    "    print(f\"requests Session creation failed {repr(e)}\")\n",
    "\n",
    "if fred_key is not None and fred_series is not None:\n",
    "    from autots.datasets.fred2 import Fred  # noqa\n",
    "    from autots.datasets.fred import get_fred_data\n",
    "\n",
    "    fred_df = get_fred_data(\n",
    "        fred_key,\n",
    "        fred_series,\n",
    "        long=False,\n",
    "        observation_start=observation_start,\n",
    "        sleep_seconds=sleep_seconds,\n",
    "    )\n",
    "    # fred_df.index = fred_df.index.tz_localize(None)\n",
    "    # dataset_lists.append(fred_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义current_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_lists = []\n",
    "if observation_end is None:\n",
    "    current_date = datetime.datetime.utcnow()\n",
    "else:\n",
    "    current_date = observation_end\n",
    "if observation_start is None:\n",
    "    # should take from observation_end but that's expected as a string\n",
    "    observation_start = datetime.datetime.utcnow() - datetime.timedelta(\n",
    "        days=365 * 6\n",
    "    )\n",
    "    observation_start = observation_start.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "try:\n",
    "    import requests\n",
    "\n",
    "    s = requests.Session()\n",
    "except Exception as e:\n",
    "    print(f\"requests Session creation failed {repr(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 从fred_data.csv 读取数据到dataset_lists\n",
    "# fred_df = pd.read_csv('fred_data.csv', index_col=0)\n",
    "import pickle\n",
    "with open('dataset_lists.pkl', 'rb') as f:\n",
    "    dataset_lists = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            forecasting  cycling  microsoft\n",
      "date                                       \n",
      "2019-02-24            1        5         43\n",
      "2019-03-03            1        7         41\n",
      "2019-03-10            0        3         43\n",
      "2019-03-17            1        3         47\n",
      "2019-03-24            1        4         43\n",
      "(260, 3)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# dataset_lists = []\n",
    "# dataset_lists.append(fred_df) # 加入数据集列表\n",
    "# 输出list列表前五行\n",
    "print(dataset_lists[0].head())\n",
    "# 打印数据集的形状\n",
    "print(dataset_lists[0].shape)\n",
    "# 打印数据集的数量\n",
    "print(len(dataset_lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_lists2 = []\n",
    "tickers = [\"MSFT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取地震数据\n",
    "dataset_lists2 = []\n",
    "if earthquake_min_magnitude is not None:\n",
    "    try:\n",
    "        str_end_time = current_date.strftime(\"%Y-%m-%d\")\n",
    "        start_date = (\n",
    "            current_date - datetime.timedelta(days=earthquake_days)\n",
    "        ).strftime(\"%Y-%m-%d\")\n",
    "        # is limited to ~1000 rows of data, ie individual earthquakes\n",
    "        ebase = \"https://earthquake.usgs.gov/fdsnws/event/1/query?\"\n",
    "        eargs = f\"format=csv&starttime={start_date}&endtime={str_end_time}&minmagnitude={earthquake_min_magnitude}\"\n",
    "        eq = pd.read_csv(ebase + eargs)\n",
    "        eq[\"time\"] = pd.to_datetime(eq[\"time\"])\n",
    "        eq[\"time\"] = eq[\"time\"].dt.tz_localize(None)\n",
    "        eq.set_index(\"time\", inplace=True)\n",
    "        global_earthquakes = eq.resample(\"1D\").agg(\n",
    "            {\"mag\": \"mean\", \"depth\": \"count\"}\n",
    "        )\n",
    "        global_earthquakes[\"mag\"] = global_earthquakes[\"mag\"].fillna(\n",
    "            earthquake_min_magnitude\n",
    "        )\n",
    "        global_earthquakes = global_earthquakes.rename(\n",
    "            columns={\n",
    "                \"mag\": \"largest_magnitude_earthquake\",\n",
    "                \"depth\": \"count_large_earthquakes\",\n",
    "            }\n",
    "        )\n",
    "        dataset_lists2.append(global_earthquakes)\n",
    "    except Exception as e:\n",
    "        print(f\"earthquake data failed: {repr(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            largest_magnitude_earthquake  count_large_earthquakes\n",
      "time                                                             \n",
      "2022-03-20                      5.433333                        3\n",
      "2022-03-21                      5.180000                        5\n",
      "2022-03-22                      5.633333                        9\n",
      "2022-03-23                      5.250000                       10\n",
      "2022-03-24                      5.166667                        3\n",
      "(699, 2)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(dataset_lists2[0].head())\n",
    "# 打印数据集的形状\n",
    "print(dataset_lists2[0].shape)\n",
    "# 打印数据集的数量\n",
    "print(len(dataset_lists2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if trends_list is not None:\n",
    "    try:\n",
    "        from pytrends.request import TrendReq\n",
    "\n",
    "        pytrends = TrendReq(hl=\"en-US\", tz=480) # 时区，ts=360的时候是美国时区,香港是480\n",
    "        pytrends.build_payload(trends_list, geo='HK') # 设置地理位置，geo='US'是美国，如果是香港就是geo='HK'\n",
    "        # pytrends.build_payload(trends_list, timeframe=\"all\")  # 'today 12-m'\n",
    "        gtrends = pytrends.interest_over_time() # 从pytrends获取数据\n",
    "        gtrends.index = gtrends.index.tz_localize(None)\n",
    "        gtrends.drop(columns=\"isPartial\", inplace=True, errors=\"ignore\")\n",
    "        dataset_lists.append(gtrends)\n",
    "    except ImportError:\n",
    "        print(\"You need to: pip install pytrends\")\n",
    "    except Exception as e:\n",
    "        print(f\"pytrends data failed: {repr(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if wikipedia_pages is not None:\n",
    "    str_start = pd.to_datetime(observation_start).strftime(\"%Y%m%d00\")\n",
    "    str_end = current_date.strftime(\"%Y%m%d00\")\n",
    "    headers = {\n",
    "        'User-Agent': 'AutoTS load_live_daily',\n",
    "    }\n",
    "    for page in wikipedia_pages:\n",
    "        try:\n",
    "            if page == \"all\":\n",
    "                url = f\"https://wikimedia.org/api/rest_v1/metrics/pageviews/aggregate/all-projects/all-access/all-agents/daily/{str_start}/{str_end}?maxlag=5\"\n",
    "            else:\n",
    "                url = f\"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/{wiki_language}.wikipedia/all-access/all-agents/{page}/daily/{str_start}/{str_end}?maxlag=5\"\n",
    "            data = s.get(url, timeout=timeout, headers=headers)\n",
    "            data_js = data.json()\n",
    "            if \"items\" not in data_js.keys():\n",
    "                print(data_js)\n",
    "            gdf = pd.DataFrame(data_js['items'])\n",
    "            gdf['date'] = pd.to_datetime(gdf['timestamp'], format=\"%Y%m%d00\")\n",
    "            gresult = gdf.set_index('date')['views'].fillna(0)\n",
    "            gresult.name = \"wiki_\" + str(page)[0:80]\n",
    "            dataset_lists.append(gresult.to_frame())\n",
    "            time.sleep(sleep_seconds)\n",
    "        except Exception as e:\n",
    "            print(f\"Wikipedia api failed with error {repr(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/all-agents/United_States/daily/2018021900/2024021800?maxlag=5\n"
     ]
    }
   ],
   "source": [
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_lists2[0].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将dataset_lists2的元素添加到dataset_lists\n",
    "dataset_lists.extend(dataset_lists2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_lists[7].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('dataset_lists.pkl', 'wb') as f:\n",
    "    pickle.dump(dataset_lists, f)\n",
    "\n",
    "# with open('dataset_lists.pkl', 'rb') as f:\n",
    "#     dataset_lists = pickle.load(f)\n",
    "\n",
    "# 保存dataset_lists2到csv文件\n",
    "# dataset_lists2[0].to_csv('dataset_lists2_0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset_lists2))\n",
    "print(dataset_lists2[4].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_lists2[2].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fred_df.to_csv('fred_data.csv', index=True, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 小心混合到表现更好的数据中的非常嘈杂的大值序列，因为它们可能会扭曲某些指标，从而获得大部分关注\n",
    "# 删除 \"volume\" 数据，因为它会扭曲 MAE（其他解决方案是将 metric_weighting 调整为 SMAPE、使用系列“权重”或预缩放数据）\n",
    "df = df[[x for x in df.columns if \"_volume\" not in x]]\n",
    "# remove dividends and stock splits as it skews metrics\n",
    "df = df[[x for x in df.columns if \"_dividends\" not in x]]\n",
    "df = df[[x for x in df.columns if \"stock_splits\" not in x]]\n",
    "# 将“wiki_all”扩展到数百万以防止 MAE 出现太大偏差\n",
    "if 'wiki_all' in df.columns:\n",
    "    df['wiki_all_millions'] = df['wiki_all'] / 1000000\n",
    "    df = df.drop(columns=['wiki_all'])\n",
    "\n",
    "# 当真实值容易估计时手动清理NaN是一种方法\n",
    "# 尽管如果你对为何它是随机的“没有好主意”，自动处理是最好的\n",
    "# 注意手动预清理显著影响验证（无论是好是坏）\n",
    "# 因为历史中的NaN时间会被度量标准跳过，但在这里添加的填充值会被评估\n",
    "if trend_list is not None:\n",
    "    for tx in trend_list:\n",
    "        if tx in df.columns:\n",
    "            df[tx] = df[tx].interpolate('akima').fillna(method='ffill', limit=30).fillna(method='bfill', limit=30)\n",
    "# 填补周末\n",
    "if tickers is not None:\n",
    "    for fx in tickers:\n",
    "        for suffix in [\"_high\", \"_low\", \"_open\", \"_close\"]:\n",
    "            fxs = (fx + suffix).lower()\n",
    "            if fxs in df.columns:\n",
    "                df[fxs] = df[fxs].interpolate('akima')\n",
    "if fred_series is not None:\n",
    "    for fx in fred_series:\n",
    "        if fx in df.columns:\n",
    "            df[fx] = df[fx].interpolate('akima')\n",
    "if weather_event_types is not None:\n",
    "    wevnt = [x for x in df.columns if \"_Events\" in x]\n",
    "    df[wevnt] = df[wevnt].mask(df[wevnt].notnull().cummax(), df[wevnt].fillna(0))\n",
    "# 这里的大部分NaN只是周末时的，当时金融系列数据没有被收集，向前填充几步是可以的\n",
    "# 部分向前填充，不向后填充\n",
    "df = df.fillna(method='ffill', limit=3)\n",
    "\n",
    "df = df[df.index.year > 1999]\n",
    "# 移除任何未来的数据\n",
    "df = df[df.index <= start_time]\n",
    "# 移除最近没有数据的序列\n",
    "df = df.dropna(axis=\"columns\", how=\"all\")\n",
    "min_cutoff_date = start_time - datetime.timedelta(days=180)\n",
    "most_recent_date = df.notna()[::-1].idxmax()\n",
    "drop_cols = most_recent_date[most_recent_date < min_cutoff_date].index.tolist()\n",
    "df = df.drop(columns=drop_cols)\n",
    "print(\n",
    "    f\"Series with most NaN: {df.head(365).isnull().sum().sort_values(ascending=False).head(5)}\"\n",
    ")\n",
    "\n",
    "# 保存这个以便在不需要等待下载的情况下重新运行，但在生产中移除这个\n",
    "df.to_csv(f\"training_data_{forecast_name}.csv\")\n",
    "else:\n",
    "df = pd.read_csv(f\"training_data_{forecast_name}.csv\", index_col=0, parse_dates=[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# future_regressor 示例，其中包含我们可以从数据和日期时间索引中收集的一些内容\n",
    "# 请注意，这只接受`wide`样式的输入数据帧\n",
    "# 这是可选的，建模不需要\n",
    "# 在包含之前也创建 Macro_micro\n",
    "regr_train, regr_fcst = create_regressor(\n",
    "    df,\n",
    "    forecast_length=forecast_length,\n",
    "    frequency=frequency,\n",
    "    drop_most_recent=drop_most_recent,\n",
    "    scale=True,\n",
    "    summarize=\"auto\",\n",
    "    backfill=\"bfill\",\n",
    "    fill_na=\"spline\",\n",
    "    holiday_countries={\"US\": None},  # requires holidays package\n",
    "    encode_holiday_type=True,\n",
    "    # datepart_method=\"simple_2\",\n",
    ")\n",
    "\n",
    "# 删除前一个 Forecast_length 行（因为这些行在回归器中丢失）\n",
    "df = df.iloc[forecast_length:]\n",
    "regr_train = regr_train.iloc[forecast_length:]\n",
    "\n",
    "print(\"data setup completed, beginning modeling\")\n",
    "\"\"\"\n",
    "Begin modeling\n",
    "\"\"\"\n",
    "\n",
    "metric_weighting = {\n",
    "    'smape_weighting': 1,\n",
    "    'mae_weighting': 3,\n",
    "    'rmse_weighting': 2,\n",
    "    'made_weighting': 1,\n",
    "    'mage_weighting': 0,\n",
    "    'mate_weighting': 0.01,\n",
    "    'mle_weighting': 1,\n",
    "    'imle_weighting': 0,\n",
    "    'spl_weighting': 5,\n",
    "    'dwae_weighting': 1,\n",
    "    'uwmse_weighting': 1,\n",
    "    'dwd_weighting': 0.1,\n",
    "    'runtime_weighting': 0.05,\n",
    "}\n",
    "\n",
    "model = AutoTS(\n",
    "    forecast_length=forecast_length,\n",
    "    frequency=frequency,\n",
    "    prediction_interval=prediction_interval,\n",
    "    ensemble=ensemble,\n",
    "    model_list=model_list,\n",
    "    transformer_list=transformer_list,\n",
    "    transformer_max_depth=transformer_max_depth,\n",
    "    max_generations=gens,\n",
    "    metric_weighting=metric_weighting,\n",
    "    initial_template=initial_template,\n",
    "    aggfunc=\"first\",\n",
    "    models_to_validate=models_to_validate,\n",
    "    model_interrupt=True,\n",
    "    num_validations=num_validations,\n",
    "    validation_method=validation_method,\n",
    "    constraint=None,\n",
    "    drop_most_recent=drop_most_recent,  # 如果最新数据不完整，也要记得增加forecast_length\n",
    "    preclean=preclean,\n",
    "    models_mode=models_mode,\n",
    "    # no_negatives=True,\n",
    "    # subset=100,\n",
    "    # prefill_na=0,\n",
    "    # remove_leading_zeroes=True,\n",
    "    # current_model_file=f\"current_model_{forecast_name}\",\n",
    "    generation_timeout=generation_timeout,\n",
    "    n_jobs=n_jobs,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "if not initial_training:\n",
    "    if evolve:\n",
    "        model.import_template(template_filename, method=\"addon\")\n",
    "    else:\n",
    "        # model.import_template(template_filename, method=\"only\")\n",
    "        model.import_best_model(template_filename)  # include_ensemble=False\n",
    "\n",
    "if evolve or initial_training:\n",
    "    model = model.fit(\n",
    "        df,\n",
    "        future_regressor=regr_train,\n",
    "        # weights='mean'\n",
    "    )\n",
    "else:\n",
    "    model.fit_data(df, future_regressor=regr_train)\n",
    "\n",
    "# save a template of best models\n",
    "if initial_training or evolve:\n",
    "    model.export_template(\n",
    "        template_filename,\n",
    "        models=\"best\",\n",
    "        n=n_export,\n",
    "        max_per_model_class=6,\n",
    "        include_results=True,\n",
    "    )\n",
    "    if archive_templates:\n",
    "        arc_file = f\"{template_filename.split('.csv')[0]}_{start_time.strftime('%Y%m%d%H%M')}.csv\"\n",
    "        model.export_template(arc_file, models=\"best\", n=1)\n",
    "\n",
    "prediction = model.predict(\n",
    "    future_regressor=regr_fcst, verbose=2, fail_on_forecast_nan=True\n",
    ")\n",
    "\n",
    "# 打印最佳模型的详细信息\n",
    "print(model)\n",
    "\n",
    "\"\"\"\n",
    "Process results\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 点预测 dataframe\n",
    "forecasts_df = prediction.forecast  # .fillna(0).round(0)\n",
    "if forecast_csv_name is not None:\n",
    "    forecasts_df.to_csv(forecast_csv_name)\n",
    "\n",
    "forecasts_upper_df = prediction.upper_forecast\n",
    "forecasts_lower_df = prediction.lower_forecast\n",
    "\n",
    "# 所有尝试的模型结果的准确性\n",
    "model_results = model.results()\n",
    "validation_results = model.results(\"validation\")\n",
    "\n",
    "print(f\"Model failure rate is {model.failure_rate() * 100:.1f}%\")\n",
    "print(f'The following model types failed completely {model.list_failed_model_types()}')\n",
    "print(\"Slowest models:\")\n",
    "print(\n",
    "    model_results[model_results[\"Ensemble\"] < 1]\n",
    "    .groupby(\"Model\")\n",
    "    .agg({\"TotalRuntimeSeconds\": [\"mean\", \"max\"]})\n",
    "    .idxmax()\n",
    ")\n",
    "\n",
    "model_parameters = json.loads(model.best_model[\"ModelParameters\"].iloc[0])\n",
    " # model.export_template(\"all_results.csv\", models='all')\n",
    "\n",
    "if graph:\n",
    "    with plt.style.context(\"bmh\"):\n",
    "        start_date = 'auto'  # '2021-01-01'\n",
    "\n",
    "        prediction.plot_grid(model.df_wide_numeric, start_date=start_date)\n",
    "        plt.show()\n",
    "\n",
    "        scores = model.best_model_per_series_mape().index.tolist()\n",
    "        scores = [x for x in scores if x in df.columns]\n",
    "        worst = scores[0:6]\n",
    "        prediction.plot_grid(model.df_wide_numeric, start_date=start_date, title=\"Worst Performing Forecasts\", cols=worst)\n",
    "        plt.show()\n",
    "\n",
    "        best = scores[-6:]\n",
    "        prediction.plot_grid(model.df_wide_numeric, start_date=start_date, title=\"Best Performing Forecasts\", cols=best)\n",
    "        plt.show()\n",
    "\n",
    "        if model.best_model_name == \"Cassandra\":\n",
    "            prediction.model.plot_components(\n",
    "                prediction, series=None, to_origin_space=True, start_date=start_date\n",
    "            )\n",
    "            plt.show()\n",
    "            prediction.model.plot_trend(\n",
    "                series=None, start_date=start_date\n",
    "            )\n",
    "            plt.show()\n",
    "    \n",
    "        ax = model.plot_per_series_mape()\n",
    "        plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "        plt.show()\n",
    "    \n",
    "        \n",
    "        if back_forecast:\n",
    "            model.plot_backforecast()\n",
    "            plt.show()\n",
    "        \n",
    "        ax = model.plot_validations()\n",
    "        plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "        plt.show()\n",
    "\n",
    "        ax = model.plot_validations(subset='best')\n",
    "        plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "        plt.show()\n",
    "\n",
    "        ax = model.plot_validations(subset='worst')\n",
    "        plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "        plt.show()\n",
    "    \n",
    "        if model.best_model_ensemble == 2:\n",
    "            plt.subplots_adjust(bottom=0.5)\n",
    "            model.plot_horizontal_transformers()\n",
    "            plt.show()\n",
    "            model.plot_horizontal_model_count()\n",
    "            plt.show()\n",
    "    \n",
    "            model.plot_horizontal()\n",
    "            plt.show()\n",
    "            # plt.savefig(\"horizontal.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    \n",
    "            if str(model_parameters[\"model_name\"]).lower() in [\"mosaic\", \"mosaic-window\"]:\n",
    "                mosaic_df = model.mosaic_to_df()\n",
    "                print(mosaic_df[mosaic_df.columns[0:5]].head(5))\n",
    "\n",
    "print(f\"Completed at system time: {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = \"赞赞\"\n",
    "z = \"牛哥\"\n",
    "\n",
    "y = i + z\n",
    "\n",
    "print(y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autots",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
