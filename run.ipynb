{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "生产实例\n",
    "\n",
    "推荐安装： pip install pytrends fredapi yfinance\n",
    "使用许多实时公共数据源构建示例生产案例。\n",
    "\n",
    "虽然此处显示了股价预测，但单独的时间序列预测并不是管理投资的推荐基础！\n",
    "\n",
    "这是一种非常固执己见的方法。\n",
    "evolution = True 允许时间序列自动适应变化。\n",
    "\n",
    "然而，它存在陷入次优位置的轻微风险。\n",
    "它可能应该与一些基本的数据健全性检查相结合。\n",
    "\n",
    "cd ./AutoTS\n",
    "conda activate py38\n",
    "nohup python production_example.py > /dev/null &\n",
    "\"\"\"\n",
    "try:  # needs to go first\n",
    "    from sklearnex import patch_sklearn\n",
    "\n",
    "    patch_sklearn()\n",
    "except Exception as e:\n",
    "    print(repr(e))\n",
    "import json\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt  # required only for graphs \n",
    "from autots import AutoTS, load_live_daily, create_regressor\n",
    "\n",
    "fred_key = 'd84151f6309da8996e4f7627d6efc026'  # https://fred.stlouisfed.org/docs/api/api_key.html\n",
    "gsa_key = 'c3bd622a-44c4-472c-92f7-de6f2423634f' # https://open.gsa.gov/api/dap/\n",
    "\n",
    "forecast_name = \"example\"\n",
    "graph = True  # 是否绘制图形\n",
    "# https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects\n",
    "frequency = (\n",
    "    \"D\"  # “infer”用于自动对齐，但特定偏移量最可靠，“D”是每日\n",
    ")\n",
    "forecast_length = 60  #  未来预测的周期数\n",
    "drop_most_recent = 1  #  是否丢弃最近的n条记录（视为不完整）\n",
    "num_validations = (\n",
    "    2  # 交叉验证运行次数。 通常越多越好但速度越慢\n",
    ")\n",
    "validation_method = \"backwards\"  # \"similarity\", \"backwards\", \"seasonal 364\"\n",
    "n_jobs = \"auto\"  # 或设置为CPU核心数\n",
    "prediction_interval = (\n",
    "    0.9  # 通过概率范围设置预测范围的上限和下限。 更大=更宽 Bigger = wider\n",
    ")\n",
    "initial_training = \"auto\"  # 在第一次运行时将其设置为 True，或者在重置时，'auto' 会查找现有模板，如果找到，则设置为 False。\n",
    "evolve = True  # 允许时间序列在每次运行中逐步演化，如果为 False，则使用固定模板\n",
    "archive_templates = True  # 保存使用时间戳的模型模板的副本\n",
    "save_location = None  # \"C:/Users/Colin/Downloads\"  # 保存模板的目录。 默认为工作目录\n",
    "template_filename = f\"autots_forecast_template_{forecast_name}.csv\"\n",
    "forecast_csv_name = None  # f\"autots_forecast_{forecast_name}.csv\" 或 None，仅写入点预测\n",
    "model_list = \"scalable\"\n",
    "transformer_list = \"fast\"  # 'superfast'\n",
    "transformer_max_depth = 5\n",
    "models_mode = \"default\"  # \"deep\", \"regressor\"\n",
    "initial_template = 'random'  # 'random' 'general+random'\n",
    "preclean = None\n",
    "{  # preclean option\n",
    "    \"fillna\": 'ffill',\n",
    "    \"transformations\": {\"0\": \"EWMAFilter\"},\n",
    "    \"transformation_params\": {\n",
    "        \"0\": {\"span\": 14},\n",
    "    },\n",
    "}\n",
    "back_forecast = False\n",
    "csv_load = False\n",
    "start_time = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing template found.\n"
     ]
    }
   ],
   "source": [
    "if save_location is not None:\n",
    "    template_filename = os.path.join(save_location, template_filename)\n",
    "    if forecast_csv_name is not None:\n",
    "        forecast_csv_name = os.path.join(save_location, forecast_csv_name)\n",
    "\n",
    "if initial_training == \"auto\":\n",
    "    initial_training = not os.path.exists(template_filename)\n",
    "    if initial_training:\n",
    "        print(\"No existing template found.\")\n",
    "    else:\n",
    "        print(\"Existing template found.\")\n",
    "\n",
    "# 根据设置设置最大代数，增加速度会更慢，但获得最高准确度的机会更大\n",
    "# 如果在 import_templates 中指定了 include_ensemble，则集成可以逐步嵌套几代\n",
    "# if include_ensemble is specified in import_templates, ensembles can progressively nest over generations\n",
    "if initial_training:\n",
    "    gens = 100\n",
    "    generation_timeout = 10000  # minutes\n",
    "    models_to_validate = 0.15\n",
    "    ensemble = [\"horizontal-max\", \"dist\", \"simple\"]  # , \"mosaic\", \"mosaic-window\", 'mlensemble'\n",
    "elif evolve:\n",
    "    gens = 500\n",
    "    generation_timeout = 300  # minutes\n",
    "    models_to_validate = 0.15\n",
    "    ensemble = [\"horizontal-max\"]  # \"mosaic\", \"mosaic-window\", \"subsample\"\n",
    "else:\n",
    "    gens = 0\n",
    "    generation_timeout = 60  # minutes\n",
    "    models_to_validate = 0.99\n",
    "    ensemble = [\"horizontal-max\", \"dist\", \"simple\"]  # \"mosaic\", \"mosaic-window\",\n",
    "\n",
    "# 如果不进化，只保存最好的模型\n",
    "if evolve:\n",
    "    n_export = 50\n",
    "else:\n",
    "    n_export = 1  # > 1 不是一个坏主意，允许一些未来的适应性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "long: bool = False\n",
    "observation_start: str = None\n",
    "observation_end: str = None\n",
    "# fred_key: str = None\n",
    "fred_series=[\"DGS10\", \"T5YIE\", \"SP500\", \"DCOILWTICO\", \"DEXUSEU\", \"WPU0911\"]\n",
    "tickers: list = [\"MSFT\"]\n",
    "trends_list: list = [\"forecasting\", \"cycling\", \"microsoft\"]\n",
    "trends_geo: str = \"US\"\n",
    "weather_data_types: list = [\"AWND\", \"WSF2\", \"TAVG\"]\n",
    "weather_stations: list = [\"USW00013960\", \"USW00014925\"]\n",
    "weather_years: int = 6\n",
    "london_air_stations: list = ['CT3', 'SK8']\n",
    "london_air_species: str = \"PM25\"\n",
    "london_air_days: int = 700\n",
    "earthquake_days: int = 700\n",
    "earthquake_min_magnitude: int = 5\n",
    "# gsa_key: str = 'c3bd622a-44c4-472c-92f7-de6f2423634f'  # https://open.gsa.gov/api/dap/\n",
    "gov_domain_list=['nasa.gov']\n",
    "gov_domain_limit: int = 600\n",
    "wikipedia_pages: list = ['Microsoft_Office', \"List_of_highest-grossing_films\"]\n",
    "wiki_language: str = \"en\"\n",
    "weather_event_types=[\"%28Z%29+Winter+Weather\", \"%28Z%29+Winter+Storm\"]\n",
    "caiso_query: str = \"ENE_SLRS\"\n",
    "timeout: float = 300.05\n",
    "sleep_seconds: int = 15\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "earthquake_min_magnitude=5\n",
    "weather_years=3\n",
    "london_air_days=700\n",
    "gov_domain_list=None\n",
    "gov_domain_limit=700\n",
    "sleep_seconds=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Begin dataset retrieval 翻译：开始数据集检索\n",
    "\"\"\"\n",
    "import os\n",
    "# 设置代理\n",
    "os.environ['HTTP_PROXY'] = \"http://127.0.0.1:10809\"\n",
    "os.environ['HTTPS_PROXY'] = \"http://127.0.0.1:10809\"\n",
    "\n",
    "if not csv_load:\n",
    "    fred_series = [\n",
    "        \"DGS10\",\n",
    "        \"T5YIE\",\n",
    "        \"SP500\",\n",
    "        \"DCOILWTICO\",\n",
    "        \"DEXUSUK\",\n",
    "        \"DEXUSEU\",\n",
    "        \"BAMLH0A0HYM2\",\n",
    "        \"DAAA\",\n",
    "        \"T10Y2Y\",\n",
    "    ]\n",
    "    tickers = [\"MSFT\", \"PG\"] # \n",
    "    trend_list = [\"forecasting\", \"msft\", \"p&g\"]\n",
    "    weather_event_types = [\"%28Z%29+Winter+Weather\", \"%28Z%29+Winter+Storm\"]\n",
    "    wikipedia_pages = ['all', 'Microsoft', \"Procter_%26_Gamble\", \"YouTube\", \"United_States\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下载数据，跳过"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sleep_seconds >= 0.5, \"sleep_seconds must be >=0.5\"\n",
    "\n",
    "dataset_lists = []\n",
    "if observation_end is None:\n",
    "    current_date = datetime.datetime.utcnow()\n",
    "else:\n",
    "    current_date = observation_end\n",
    "if observation_start is None:\n",
    "    # should take from observation_end but that's expected as a string\n",
    "    observation_start = datetime.datetime.utcnow() - datetime.timedelta(\n",
    "        days=365 * 6\n",
    "    )\n",
    "    observation_start = observation_start.strftime(\"%Y-%m-%d\")\n",
    "try:\n",
    "    import requests\n",
    "\n",
    "    s = requests.Session()\n",
    "except Exception as e:\n",
    "    print(f\"requests Session creation failed {repr(e)}\")\n",
    "\n",
    "if fred_key is not None and fred_series is not None:\n",
    "    from autots.datasets.fred2 import Fred  # noqa\n",
    "    from autots.datasets.fred import get_fred_data\n",
    "\n",
    "    fred_df = get_fred_data(\n",
    "        fred_key,\n",
    "        fred_series,\n",
    "        long=False,\n",
    "        observation_start=observation_start,\n",
    "        sleep_seconds=sleep_seconds,\n",
    "    )\n",
    "    # fred_df.index = fred_df.index.tz_localize(None)\n",
    "    # dataset_lists.append(fred_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义current_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_lists = []\n",
    "if observation_end is None:\n",
    "    current_date = datetime.datetime.utcnow()\n",
    "else:\n",
    "    current_date = observation_end\n",
    "if observation_start is None:\n",
    "    # should take from observation_end but that's expected as a string\n",
    "    observation_start = datetime.datetime.utcnow() - datetime.timedelta(\n",
    "        days=365 * 6\n",
    "    )\n",
    "    observation_start = observation_start.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "try:\n",
    "    import requests\n",
    "\n",
    "    s = requests.Session()\n",
    "except Exception as e:\n",
    "    print(f\"requests Session creation failed {repr(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 从fred_data.csv 读取数据到dataset_lists\n",
    "# fred_df = pd.read_csv('fred_data.csv', index_col=0)\n",
    "import pickle\n",
    "with open('dataset_lists.pkl', 'rb') as f:\n",
    "    dataset_lists = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            USW00013960_AWND  USW00013960_TAVG  USW00013960_WSF2\n",
      "DATE                                                            \n",
      "2018-03-01              9.62                60              21.9\n",
      "2018-03-02              6.71                56              14.1\n",
      "2018-03-03             12.30                62              21.9\n",
      "2018-03-04             13.42                67              25.1\n",
      "2018-03-05             12.30                70              25.9\n",
      "(1508, 7)\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# dataset_lists = []\n",
    "# dataset_lists.append(fred_df) # 加入数据集列表\n",
    "# 输出list列表前五行\n",
    "print(dataset_lists[3].head())\n",
    "# 打印数据集的形状\n",
    "print(dataset_lists[1].shape)\n",
    "# 打印数据集的数量\n",
    "print(len(dataset_lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 series downloaded.\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "# 首先确保所有数据集的索引都转换为统一的日期时间格式\n",
    "dataset_lists = [dataset.set_index(pd.to_datetime(dataset.index)) for dataset in dataset_lists]\n",
    "\n",
    "\n",
    "df = reduce(\n",
    "    lambda x, y: pd.merge(x, y, left_index=True, right_index=True, how=\"outer\"), # 合并数据集\n",
    "    dataset_lists,\n",
    ")\n",
    "print(f\"{df.shape[1]} series downloaded.\")\n",
    "s.close()\n",
    "df.index.name = \"datetime\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long = df.reset_index(drop=False).melt(\n",
    "    id_vars=['datetime'], var_name='series_id', value_name='value'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tickers is not None:\n",
    "    for fx in tickers:\n",
    "        for suffix in [\"_high\", \"_low\", \"_open\", \"_close\"]:\n",
    "            fxs = (fx + suffix).lower()\n",
    "            if fxs in df.columns:\n",
    "                df[fxs] = df[fxs].interpolate('akima')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fred_series is not None:\n",
    "    for fx in fred_series:\n",
    "        if fx in df.columns:\n",
    "            df[fx] = df[fx].interpolate('akima')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.ffill(limit=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.index <= start_time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[x for x in df.columns if \"_volume\" not in x]]\n",
    "# 取消股息和股票分割，因为它会扭曲指标\n",
    "df = df[[x for x in df.columns if \"_dividends\" not in x]]\n",
    "df = df[[x for x in df.columns if \"stock_splits\" not in x]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"training_data_{forecast_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('dataset_lists.pkl', 'wb') as f:\n",
    "    pickle.dump(dataset_lists, f)\n",
    "\n",
    "# with open('dataset_lists.pkl', 'rb') as f:\n",
    "#     dataset_lists = pickle.load(f)\n",
    "\n",
    "# 保存dataset_lists2到csv文件\n",
    "# dataset_lists2[0].to_csv('dataset_lists2_0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 小心混合到表现更好的数据中的非常嘈杂的大值序列，因为它们可能会扭曲某些指标，从而获得大部分关注\n",
    "# 删除 \"volume\" 数据，因为它会扭曲 MAE（其他解决方案是将 metric_weighting 调整为 SMAPE、使用系列“权重”或预缩放数据）\n",
    "df = df[[x for x in df.columns if \"_volume\" not in x]]\n",
    "# remove dividends and stock splits as it skews metrics\n",
    "df = df[[x for x in df.columns if \"_dividends\" not in x]]\n",
    "df = df[[x for x in df.columns if \"stock_splits\" not in x]]\n",
    "# 将“wiki_all”扩展到数百万以防止 MAE 出现太大偏差\n",
    "if 'wiki_all' in df.columns:\n",
    "    df['wiki_all_millions'] = df['wiki_all'] / 1000000\n",
    "    df = df.drop(columns=['wiki_all'])\n",
    "\n",
    "# 当真实值容易估计时手动清理NaN是一种方法\n",
    "# 尽管如果你对为何它是随机的“没有好主意”，自动处理是最好的\n",
    "# 注意手动预清理显著影响验证（无论是好是坏）\n",
    "# 因为历史中的NaN时间会被度量标准跳过，但在这里添加的填充值会被评估\n",
    "if trend_list is not None:\n",
    "    for tx in trend_list:\n",
    "        if tx in df.columns:\n",
    "            df[tx] = df[tx].interpolate('akima').fillna(method='ffill', limit=30).fillna(method='bfill', limit=30)\n",
    "# 填补周末\n",
    "if tickers is not None:\n",
    "    for fx in tickers:\n",
    "        for suffix in [\"_high\", \"_low\", \"_open\", \"_close\"]:\n",
    "            fxs = (fx + suffix).lower()\n",
    "            if fxs in df.columns:\n",
    "                df[fxs] = df[fxs].interpolate('akima')\n",
    "if fred_series is not None:\n",
    "    for fx in fred_series:\n",
    "        if fx in df.columns:\n",
    "            df[fx] = df[fx].interpolate('akima')\n",
    "if weather_event_types is not None:\n",
    "    wevnt = [x for x in df.columns if \"_Events\" in x]\n",
    "    df[wevnt] = df[wevnt].mask(df[wevnt].notnull().cummax(), df[wevnt].fillna(0))\n",
    "# 这里的大部分NaN只是周末时的，当时金融系列数据没有被收集，向前填充几步是可以的\n",
    "# 部分向前填充，不向后填充\n",
    "df = df.fillna(method='ffill', limit=3)\n",
    "\n",
    "df = df[df.index.year > 1999]\n",
    "# 移除任何未来的数据\n",
    "df = df[df.index <= start_time]\n",
    "# 移除最近没有数据的序列\n",
    "df = df.dropna(axis=\"columns\", how=\"all\")\n",
    "min_cutoff_date = start_time - datetime.timedelta(days=180)\n",
    "most_recent_date = df.notna()[::-1].idxmax()\n",
    "drop_cols = most_recent_date[most_recent_date < min_cutoff_date].index.tolist()\n",
    "df = df.drop(columns=drop_cols)\n",
    "print(\n",
    "    f\"Series with most NaN: {df.head(365).isnull().sum().sort_values(ascending=False).head(5)}\"\n",
    ")\n",
    "\n",
    "# 保存这个以便在不需要等待下载的情况下重新运行，但在生产中移除这个\n",
    "df.to_csv(f\"training_data_{forecast_name}.csv\")\n",
    "else:\n",
    "df = pd.read_csv(f\"training_data_{forecast_name}.csv\", index_col=0, parse_dates=[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# future_regressor 示例，其中包含我们可以从数据和日期时间索引中收集的一些内容\n",
    "# 请注意，这只接受`wide`样式的输入数据帧\n",
    "# 这是可选的，建模不需要\n",
    "# 在包含之前也创建 Macro_micro\n",
    "regr_train, regr_fcst = create_regressor(\n",
    "    df,\n",
    "    forecast_length=forecast_length,\n",
    "    frequency=frequency,\n",
    "    drop_most_recent=drop_most_recent,\n",
    "    scale=True,\n",
    "    summarize=\"auto\",\n",
    "    backfill=\"bfill\",\n",
    "    fill_na=\"spline\",\n",
    "    holiday_countries={\"US\": None},  # requires holidays package\n",
    "    encode_holiday_type=True,\n",
    "    # datepart_method=\"simple_2\",\n",
    ")\n",
    "\n",
    "# 删除前一个 Forecast_length 行（因为这些行在回归器中丢失）\n",
    "df = df.iloc[forecast_length:]\n",
    "regr_train = regr_train.iloc[forecast_length:]\n",
    "\n",
    "print(\"data setup completed, beginning modeling\")\n",
    "\"\"\"\n",
    "Begin modeling\n",
    "\"\"\"\n",
    "\n",
    "metric_weighting = {\n",
    "    'smape_weighting': 1,\n",
    "    'mae_weighting': 3,\n",
    "    'rmse_weighting': 2,\n",
    "    'made_weighting': 1,\n",
    "    'mage_weighting': 0,\n",
    "    'mate_weighting': 0.01,\n",
    "    'mle_weighting': 1,\n",
    "    'imle_weighting': 0,\n",
    "    'spl_weighting': 5,\n",
    "    'dwae_weighting': 1,\n",
    "    'uwmse_weighting': 1,\n",
    "    'dwd_weighting': 0.1,\n",
    "    'runtime_weighting': 0.05,\n",
    "}\n",
    "\n",
    "model = AutoTS(\n",
    "    forecast_length=forecast_length,\n",
    "    frequency=frequency,\n",
    "    prediction_interval=prediction_interval,\n",
    "    ensemble=ensemble,\n",
    "    model_list=model_list,\n",
    "    transformer_list=transformer_list,\n",
    "    transformer_max_depth=transformer_max_depth,\n",
    "    max_generations=gens,\n",
    "    metric_weighting=metric_weighting,\n",
    "    initial_template=initial_template,\n",
    "    aggfunc=\"first\",\n",
    "    models_to_validate=models_to_validate,\n",
    "    model_interrupt=True,\n",
    "    num_validations=num_validations,\n",
    "    validation_method=validation_method,\n",
    "    constraint=None,\n",
    "    drop_most_recent=drop_most_recent,  # 如果最新数据不完整，也要记得增加forecast_length\n",
    "    preclean=preclean,\n",
    "    models_mode=models_mode,\n",
    "    # no_negatives=True,\n",
    "    # subset=100,\n",
    "    # prefill_na=0,\n",
    "    # remove_leading_zeroes=True,\n",
    "    # current_model_file=f\"current_model_{forecast_name}\",\n",
    "    generation_timeout=generation_timeout,\n",
    "    n_jobs=n_jobs,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "if not initial_training:\n",
    "    if evolve:\n",
    "        model.import_template(template_filename, method=\"addon\")\n",
    "    else:\n",
    "        # model.import_template(template_filename, method=\"only\")\n",
    "        model.import_best_model(template_filename)  # include_ensemble=False\n",
    "\n",
    "if evolve or initial_training:\n",
    "    model = model.fit(\n",
    "        df,\n",
    "        future_regressor=regr_train,\n",
    "        # weights='mean'\n",
    "    )\n",
    "else:\n",
    "    model.fit_data(df, future_regressor=regr_train)\n",
    "\n",
    "# save a template of best models\n",
    "if initial_training or evolve:\n",
    "    model.export_template(\n",
    "        template_filename,\n",
    "        models=\"best\",\n",
    "        n=n_export,\n",
    "        max_per_model_class=6,\n",
    "        include_results=True,\n",
    "    )\n",
    "    if archive_templates:\n",
    "        arc_file = f\"{template_filename.split('.csv')[0]}_{start_time.strftime('%Y%m%d%H%M')}.csv\"\n",
    "        model.export_template(arc_file, models=\"best\", n=1)\n",
    "\n",
    "prediction = model.predict(\n",
    "    future_regressor=regr_fcst, verbose=2, fail_on_forecast_nan=True\n",
    ")\n",
    "\n",
    "# 打印最佳模型的详细信息\n",
    "print(model)\n",
    "\n",
    "\"\"\"\n",
    "Process results\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 点预测 dataframe\n",
    "forecasts_df = prediction.forecast  # .fillna(0).round(0)\n",
    "if forecast_csv_name is not None:\n",
    "    forecasts_df.to_csv(forecast_csv_name)\n",
    "\n",
    "forecasts_upper_df = prediction.upper_forecast\n",
    "forecasts_lower_df = prediction.lower_forecast\n",
    "\n",
    "# 所有尝试的模型结果的准确性\n",
    "model_results = model.results()\n",
    "validation_results = model.results(\"validation\")\n",
    "\n",
    "print(f\"Model failure rate is {model.failure_rate() * 100:.1f}%\")\n",
    "print(f'The following model types failed completely {model.list_failed_model_types()}')\n",
    "print(\"Slowest models:\")\n",
    "print(\n",
    "    model_results[model_results[\"Ensemble\"] < 1]\n",
    "    .groupby(\"Model\")\n",
    "    .agg({\"TotalRuntimeSeconds\": [\"mean\", \"max\"]})\n",
    "    .idxmax()\n",
    ")\n",
    "\n",
    "model_parameters = json.loads(model.best_model[\"ModelParameters\"].iloc[0])\n",
    " # model.export_template(\"all_results.csv\", models='all')\n",
    "\n",
    "if graph:\n",
    "    with plt.style.context(\"bmh\"):\n",
    "        start_date = 'auto'  # '2021-01-01'\n",
    "\n",
    "        prediction.plot_grid(model.df_wide_numeric, start_date=start_date)\n",
    "        plt.show()\n",
    "\n",
    "        scores = model.best_model_per_series_mape().index.tolist()\n",
    "        scores = [x for x in scores if x in df.columns]\n",
    "        worst = scores[0:6]\n",
    "        prediction.plot_grid(model.df_wide_numeric, start_date=start_date, title=\"Worst Performing Forecasts\", cols=worst)\n",
    "        plt.show()\n",
    "\n",
    "        best = scores[-6:]\n",
    "        prediction.plot_grid(model.df_wide_numeric, start_date=start_date, title=\"Best Performing Forecasts\", cols=best)\n",
    "        plt.show()\n",
    "\n",
    "        if model.best_model_name == \"Cassandra\":\n",
    "            prediction.model.plot_components(\n",
    "                prediction, series=None, to_origin_space=True, start_date=start_date\n",
    "            )\n",
    "            plt.show()\n",
    "            prediction.model.plot_trend(\n",
    "                series=None, start_date=start_date\n",
    "            )\n",
    "            plt.show()\n",
    "    \n",
    "        ax = model.plot_per_series_mape()\n",
    "        plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "        plt.show()\n",
    "    \n",
    "        \n",
    "        if back_forecast:\n",
    "            model.plot_backforecast()\n",
    "            plt.show()\n",
    "        \n",
    "        ax = model.plot_validations()\n",
    "        plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "        plt.show()\n",
    "\n",
    "        ax = model.plot_validations(subset='best')\n",
    "        plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "        plt.show()\n",
    "\n",
    "        ax = model.plot_validations(subset='worst')\n",
    "        plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "        plt.show()\n",
    "    \n",
    "        if model.best_model_ensemble == 2:\n",
    "            plt.subplots_adjust(bottom=0.5)\n",
    "            model.plot_horizontal_transformers()\n",
    "            plt.show()\n",
    "            model.plot_horizontal_model_count()\n",
    "            plt.show()\n",
    "    \n",
    "            model.plot_horizontal()\n",
    "            plt.show()\n",
    "            # plt.savefig(\"horizontal.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    \n",
    "            if str(model_parameters[\"model_name\"]).lower() in [\"mosaic\", \"mosaic-window\"]:\n",
    "                mosaic_df = model.mosaic_to_df()\n",
    "                print(mosaic_df[mosaic_df.columns[0:5]].head(5))\n",
    "\n",
    "print(f\"Completed at system time: {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = \"赞赞\"\n",
    "z = \"牛哥\"\n",
    "\n",
    "y = i + z\n",
    "\n",
    "print(y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autots",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
