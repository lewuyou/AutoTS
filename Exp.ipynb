{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing template found.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "生产实例\n",
    "\n",
    "推荐安装： pip install pytrends fredapi yfinance\n",
    "使用许多实时公共数据源构建示例生产案例。\n",
    "\n",
    "虽然此处显示了股价预测，但单独的时间序列预测并不是管理投资的推荐基础！\n",
    "\n",
    "这是一种非常固执己见的方法。\n",
    "evolution = True 允许时间序列自动适应变化。\n",
    "\n",
    "然而，它存在陷入次优位置的轻微风险。\n",
    "它可能应该与一些基本的数据健全性检查相结合。\n",
    "\n",
    "cd ./AutoTS\n",
    "conda activate py38\n",
    "nohup python production_example.py > /dev/null &\n",
    "\"\"\"\n",
    "try:  # needs to go first\n",
    "    from sklearnex import patch_sklearn\n",
    "\n",
    "    patch_sklearn()\n",
    "except Exception as e:\n",
    "    print(repr(e))\n",
    "import json\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt  # required only for graphs \n",
    "from autots import AutoTS, load_live_daily, create_regressor\n",
    "\n",
    "fred_key = 'd84151f6309da8996e4f7627d6efc026'  # https://fred.stlouisfed.org/docs/api/api_key.html\n",
    "gsa_key = None\n",
    "\n",
    "forecast_name = \"Stock2024\"\n",
    "graph = True  # 是否绘制图形\n",
    "# https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects\n",
    "frequency = (\n",
    "    \"D\"  # “infer”用于自动对齐，但特定偏移量最可靠，“D”是每日\n",
    ")\n",
    "forecast_length = 30  #  未来预测的周期数\n",
    "drop_most_recent = 1  #  是否丢弃最近的n条记录（视为不完整）\n",
    "num_validations = (\n",
    "    2  # 交叉验证运行次数。 通常越多越好但速度越慢\n",
    ")\n",
    "validation_method = \"backwards\"  # \"similarity\", \"backwards\", \"seasonal 364\"\n",
    "n_jobs = \"auto\"  # 或设置为CPU核心数\n",
    "prediction_interval = (\n",
    "    0.9  # 通过概率范围设置预测范围的上限和下限。 更大=更宽 Bigger = wider\n",
    ")\n",
    "initial_training = \"auto\"  # 在第一次运行时将其设置为 True，或者在重置时，'auto' 会查找现有模板，如果找到，则设置为 False。\n",
    "evolve = True  # 允许时间序列在每次运行中逐步演化，如果为 False，则使用固定模板\n",
    "archive_templates = True  # 保存使用时间戳的模型模板的副本\n",
    "save_location = None  # \"C:/Users/Colin/Downloads\"  # 保存模板的目录。 默认为工作目录\n",
    "template_filename = f\"autots_forecast_template_{forecast_name}.csv\"\n",
    "forecast_csv_name = f\"autots_forecast_{forecast_name}.csv\"  # f\"autots_forecast_{forecast_name}.csv\"  # or None, point forecast only is written\n",
    "model_list = 'fast_parallel'\n",
    "transformer_list = \"fast\"  # 'superfast'\n",
    "transformer_max_depth = 5\n",
    "models_mode = \"default\"  # \"deep\", \"regressor\"\n",
    "initial_template = 'random'  # 'random' 'general+random'\n",
    "preclean = None\n",
    "{  # preclean option\n",
    "    \"fillna\": 'ffill',\n",
    "    \"transformations\": {\"0\": \"EWMAFilter\"},\n",
    "    \"transformation_params\": {\n",
    "        \"0\": {\"span\": 14},\n",
    "    },\n",
    "}\n",
    "back_forecast = False\n",
    "csv_load = False\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "\n",
    "if save_location is not None:\n",
    "    template_filename = os.path.join(save_location, template_filename)\n",
    "    if forecast_csv_name is not None:\n",
    "        forecast_csv_name = os.path.join(save_location, forecast_csv_name)\n",
    "\n",
    "if initial_training == \"auto\":\n",
    "    initial_training = not os.path.exists(template_filename)\n",
    "    if initial_training:\n",
    "        print(\"No existing template found.\")\n",
    "    else:\n",
    "        print(\"Existing template found.\")\n",
    "\n",
    "# 根据设置设置最大代数，增加速度会更慢，但获得最高准确度的机会更大\n",
    "# 如果在 import_templates 中指定了 include_ensemble，则集成可以逐步嵌套几代\n",
    "# if include_ensemble is specified in import_templates, ensembles can progressively nest over generations\n",
    "if initial_training:\n",
    "    gens = 100\n",
    "    generation_timeout = 10000  # minutes\n",
    "    models_to_validate = 0.15\n",
    "    ensemble = [\"horizontal-max\", \"dist\", \"simple\"]  # , \"mosaic\", \"mosaic-window\", 'mlensemble'\n",
    "elif evolve:\n",
    "    gens = 500\n",
    "    generation_timeout = 300  # minutes\n",
    "    models_to_validate = 0.15\n",
    "    ensemble = [\"horizontal-max\"]  # \"mosaic\", \"mosaic-window\", \"subsample\"\n",
    "else:\n",
    "    gens = 0\n",
    "    generation_timeout = 60  # minutes\n",
    "    models_to_validate = 0.99\n",
    "    ensemble = [\"horizontal-max\", \"dist\", \"simple\"]  # \"mosaic\", \"mosaic-window\",\n",
    "\n",
    "# 如果不进化，只保存最好的模型\n",
    "if evolve:\n",
    "    n_export = 50\n",
    "else:\n",
    "    n_export = 1  # > 1 不是一个坏主意，允许一些未来的适应性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Begin dataset retrieval\n",
    "\"\"\"\n",
    "if not csv_load:\n",
    "    fred_series = [\n",
    "        \"DGS10\",\n",
    "        \"T5YIE\",\n",
    "        \"SP500\",\n",
    "        \"DCOILWTICO\",\n",
    "        \"DEXUSEU\",\n",
    "        \"BAMLH0A0HYM2\",\n",
    "        \"DAAA\",\n",
    "        \"DEXUSUK\",\n",
    "        \"T10Y2Y\",\n",
    "    ]\n",
    "    tickers = [\"MSFT\", \"PG\"]\n",
    "    trend_list = [\"forecasting\", \"msft\", \"p&g\"]\n",
    "    weather_event_types = [\"%28Z%29+Winter+Weather\", \"%28Z%29+Winter+Storm\"]\n",
    "    wikipedia_pages = ['all', 'Microsoft', \"Procter_%26_Gamble\", \"YouTube\", \"United_States\"]\n",
    "    df = load_live_daily(\n",
    "        long=False,\n",
    "        fred_key=fred_key,\n",
    "        fred_series=fred_series,\n",
    "        tickers=tickers,\n",
    "        trends_list=None, # 从谷歌趋势中获取数据, 设置为None以跳过\n",
    "        earthquake_min_magnitude=None, # 地震数据，设置为None以跳过\n",
    "        weather_stations=None, # 天气数据，设置为None以跳过\n",
    "        weather_years=3,\n",
    "        london_air_stations=None, # 伦敦空气质量，设置为None以跳过\n",
    "        london_air_days=700,\n",
    "        wikipedia_pages=None, # 维基百科流量，设置为None以跳过\n",
    "        gsa_key=gsa_key, \n",
    "        gov_domain_list=None,  # 政府网站流量，设置为None以跳过\n",
    "        gov_domain_limit=700,\n",
    "        weather_event_types=None, # 严重天气事件，设置为None以跳过\n",
    "        caiso_query=None, # 加利福尼亚用电数据，设置为None以跳过\n",
    "        sleep_seconds=15,\n",
    "    )\n",
    "    # 小心混合到表现更好的数据中的非常嘈杂的大值序列，因为它们可能会扭曲某些指标，从而获得大部分关注\n",
    "    # 删除 \"volume\" 数据，因为它会扭曲 MAE（其他解决方案是将 metric_weighting 调整为 SMAPE、使用系列“权重”或预缩放数据）\n",
    "    df = df[[x for x in df.columns if \"_volume\" not in x]]\n",
    "    # 取消股息和股票分割，因为它会扭曲指标\n",
    "    df = df[[x for x in df.columns if \"_dividends\" not in x]]\n",
    "    df = df[[x for x in df.columns if \"stock_splits\" not in x]]\n",
    "    # 将“wiki_all” 除以 1000000，以防止 MAE 出现太大偏差\n",
    "    if 'wiki_all' in df.columns:\n",
    "        df['wiki_all_millions'] = df['wiki_all'] / 1000000\n",
    "        df = df.drop(columns=['wiki_all'])\n",
    "    \n",
    "    # 当真实值容易估计时手动清理NaN是一种方法\n",
    "    # 尽管如果你对为何它是随机的“没有好主意”，自动处理是最好的\n",
    "    # 注意手动预清理显著影响验证（无论是好是坏）\n",
    "    # 因为历史中的NaN时间会被度量标准跳过，但在这里添加的填充值会被评估\n",
    "    \n",
    "    # 谷歌趋势\n",
    "    if trend_list is not None: \n",
    "        for tx in trend_list:\n",
    "            if tx in df.columns:\n",
    "                df[tx] = df[tx].interpolate('akima').ffill(limit=30).bfill(limit=30)\n",
    "    # 使用平滑曲线填补周末NAN值\n",
    "    if tickers is not None:\n",
    "        for fx in tickers:\n",
    "            for suffix in [\"_high\", \"_low\", \"_open\", \"_close\"]:\n",
    "                fxs = (fx + suffix).lower()\n",
    "                if fxs in df.columns:\n",
    "                    df[fxs] = df[fxs].interpolate('akima')\n",
    "    if fred_series is not None:\n",
    "        for fx in fred_series:\n",
    "            if fx in df.columns:\n",
    "                df[fx] = df[fx].interpolate('akima')\n",
    "    if weather_event_types is not None:\n",
    "        wevnt = [x for x in df.columns if \"_Events\" in x]\n",
    "        df[wevnt] = df[wevnt].mask(df[wevnt].notnull().cummax(), df[wevnt].fillna(0))\n",
    "    # most of the NaN here are just weekends, when financial series aren't collected, ffill of a few steps is fine\n",
    "    # partial forward fill, no back fill\n",
    "    # 剩下数据使用前向填充（forward fill）方法来复制 df 中的缺失值（NaN），向前复制 最多3个值\n",
    "    df = df.ffill(limit=3)\n",
    "    # 删除小于 2000 年的数据\n",
    "    df = df[df.index.year > 1999]\n",
    "    # 移除任何未来的数据\n",
    "    df = df[df.index <= start_time]\n",
    "    # 移除数据全是nan的列\n",
    "    df = df.dropna(axis=\"columns\", how=\"all\")\n",
    "    # 比现在早180天的日期\n",
    "    min_cutoff_date = start_time - datetime.timedelta(days=180)\n",
    "    # 找到最近的非nan日期\n",
    "    most_recent_date = df.notna()[::-1].idxmax()\n",
    "    # 筛选所有180天以来没有新数据的列，并将这些列的名称存储在列表 \n",
    "    drop_cols = most_recent_date[most_recent_date < min_cutoff_date].index.tolist()\n",
    "    # 丢弃这些长期不更新数据的列\n",
    "    df = df.drop(columns=drop_cols)\n",
    "    print(\n",
    "        f\"Series with most NaN: {df.head(365).isnull().sum().sort_values(ascending=False).head(5)}\"\n",
    "    )\n",
    "\n",
    "    # 保存这个以便在不需要等待下载的情况下重新运行，但在生产中移除这个\n",
    "    df.to_csv(f\"training_data_{forecast_name}.csv\")\n",
    "else:\n",
    "    df = pd.read_csv(f\"training_data_{forecast_name}.csv\", index_col=0, parse_dates=[0])\n",
    "\n",
    "# future_regressor 示例，其中包含我们可以从数据和日期时间索引中收集的一些内容\n",
    "# 请注意，这只接受`wide`样式的输入数据帧\n",
    "# 这是可选的，建模不需要\n",
    "# 在包含之前也创建 Macro_microd\n",
    "'''\n",
    "create_regressor\n",
    "1.将数据转换为宽格式\n",
    "2.删除最近的forecast_length行\n",
    "3.用nan填充索引中缺失的日期\n",
    "4.将所有列转换为数值类型\n",
    "5.标准化数据\n",
    "6.将数据降维到10维\n",
    "7.填充缺失日期（已填充）对应的数据\n",
    "8.将假期编码为二进制\n",
    "9.返回结尾部分forecast_length(预测长度)的数据\n",
    "10.预测部分数据重置索引，时间设置为最后日期往后\n",
    "11.regressor_train 数据时间索引整体推迟60天(forecast_length)\n",
    "regressor_train + regressor_forecast 的数据等于原来的数据日期整体往后移动60天\n",
    "12.对因为数据移位产生的 NaN 进行填充,使用bfill方法,如果使用ets或者datepartregression,\n",
    ",则调用model_forecast对空缺数据进行预测。\n",
    "'''\n",
    "regr_train, regr_fcst = create_regressor(\n",
    "    df,\n",
    "    forecast_length=forecast_length,\n",
    "    frequency=frequency,\n",
    "    drop_most_recent=drop_most_recent,\n",
    "    scale=True,\n",
    "    summarize=\"auto\", # 数据降维，如果auto那么使用\"feature_agglomeration\"汇聚成10维,如果不是auto就是25维。\n",
    "    backfill=\"bfill\", # 处理因为数据移位产生的 NaN 进行填充的方法\n",
    "    fill_na=\"spline\", # \"spline\", \"ffill\", \"bfill\"在数据中预填充 NA 的方法，与其他地方可用的方法相同\n",
    "    holiday_countries= None,  # {\"CN\": None} 假期不准，利用股市数据进行预测\n",
    "    encode_holiday_type=True, # 如果为 True，则返回每个假期的列，仅适用于假期套餐国家/地区假期（不适用于检测器）\n",
    "    # datepart_method=\"simple_2\",\n",
    ")\n",
    "\n",
    "# 删除前一个 Forecast_length 行（因为这些行在create_regressor中丢失）\n",
    "df = df.iloc[forecast_length:]\n",
    "regr_train = regr_train.iloc[forecast_length:]\n",
    "\n",
    "print(\"data setup completed, beginning modeling\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autots",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
