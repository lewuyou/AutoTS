{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive # 挂载谷歌云盘\n",
    "# drive.mount('/content/drive')\n",
    "# !nvidia-smi # 显示显卡信息\n",
    "# ''' 符号%代表一直生效，！代表执行完立马结束，不会生效，所以进入目录用% '''\n",
    "# %cd /content/drive/MyDrive/timeSerise\n",
    "# ''' 支持的 常用命令1.ls  2.wget  3.gdoint(int(int(int(w))))n  4.mkdir  5.pwd '''\n",
    "# !ls\n",
    "# !pip install pyti\n",
    "# !pip install akshare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\ts2\\lib\\site-packages\\akshare\\__init__.py:2724: UserWarning: 为了支持更多特性，请将 Pandas 升级到 2.1.0 及以上版本！\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from data_provider.data_creat import *\n",
    "import akshare as ak\n",
    "from datetime import datetime\n",
    "from pypinyin import lazy_pinyin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    '''数据获取'''\n",
    "    fuquan = 'hfq'# 设置复权方式,adjust=空选择的不复权，qfq是前复权，应该用hfq后复权来进行量化分析\n",
    "    period = 'daily' # 拉取时间周期{'daily', 'weekly', 'monthly'}\n",
    "    start_date = '20151201'  # 20151201   20221021  下载数据的开始日期,1就是公司上市时间\n",
    "    end_date = '20231220'  # 下载数据的结束日期,如果1则到最后一天,如果-1是昨天.\n",
    "    start_date_time = pd.to_datetime('2015-12-01')\n",
    "    end_date_time = pd.to_datetime('2023-12-20')\n",
    "    label_n = 5 # 预测未来连续多少天的收益率\n",
    "    root_path = './dataset/'\n",
    "    file_info_list = [(\"shanghai-air-quality.csv\", [0]),  # 第二个参数是获取的列索引\n",
    "                      (\"中国十年期国债收益率历史数据.csv\", [0]),\n",
    "                      (\"美国十年期国债收益率历史数据.csv\", [0]),\n",
    "                      (\"WTI原油期货历史数据.csv\", [0]),\n",
    "                      (\"黄金期货历史数据.csv\", [0]),\n",
    "                      (\"USD_CNH历史数据.csv\", [0]),\n",
    "                      (\"香港恒生指数历史数据.csv\", [0]),\n",
    "                      ]    \n",
    "    # 预测目标y是否替换成1或者0\n",
    "    zhangfu = 0.10  # 预测涨幅大于等于3%的为1，小于3%的为0\n",
    "    label_ch = True  # 如果是True ，预测n天以后上涨大于变量zhangfu为1，小于为0\n",
    "\n",
    "    # 是否合并全部股票数据\n",
    "    all = False\n",
    "    data_addzero = 60 # 当all = True时，用于训练的数据集分割，前面补0的长度\n",
    "    \n",
    "    # 数据修剪\n",
    "    start = 103 # 删除前24行（start=25），因为macd算不出来\n",
    "    end = 0 # (end = label_n if all else 0)删除最后部分需要预测天数label_n的数据，算出来是0.如果单只股票预测，那么不需要删除\n",
    "    final_data_feat =  ['index', 'Volume','Tom_Chg'] # 删除不需要列的标签\n",
    "\n",
    "# 创建参数对象\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_data(file_name, column_indices, root_path=args.root_path, start_date_time=args.start_date_time, end_date_time=args.end_date_time):\n",
    "    # 读取数据\n",
    "    df = pd.read_csv(root_path + file_name)\n",
    "    print(f\"{file_name} 数据读取成功\")\n",
    "    \n",
    "    # 获取文件名前四个字符并转换为拼音\n",
    "    file_prefix = ''.join(lazy_pinyin(file_name[:4]))\n",
    "    \n",
    "    # 更新列名，第一列改为'date'，其余列名如果是汉字也转换为拼音并前加上文件名前缀和下划线\n",
    "    new_columns = ['date'] + [f\"{file_prefix}_{''.join(lazy_pinyin(col))}\" for col in df.columns[1:]]\n",
    "    df.columns = new_columns\n",
    "    \n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    df = df.set_index('date')\n",
    "    df = df.tz_localize(None)\n",
    "    \n",
    "    # 保留指定索引的列\n",
    "    selected_columns = [df.columns[i] for i in column_indices]\n",
    "    df = df[selected_columns]\n",
    "    \n",
    "    # 转换所有选中列为数值，移除逗号\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == 'object' and df[column].str.contains(',').any():\n",
    "            df[column] = df[column].str.replace(',', '').astype(float) / 1000\n",
    "    \n",
    "    # 过滤数据\n",
    "    filtered_df = df[(df.index >= start_date_time) & (df.index <= end_date_time)]\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shanghai-air-quality.csv 数据读取成功\n",
      "中国十年期国债收益率历史数据.csv 数据读取成功\n",
      "美国十年期国债收益率历史数据.csv 数据读取成功\n",
      "WTI原油期货历史数据.csv 数据读取成功\n",
      "黄金期货历史数据.csv 数据读取成功\n",
      "USD_CNH历史数据.csv 数据读取成功\n",
      "香港恒生指数历史数据.csv 数据读取成功\n"
     ]
    }
   ],
   "source": [
    "dataframe_list = []\n",
    "\n",
    "for file_name, column_indices in args.file_info_list:\n",
    "    df = read_csv_data(file_name, column_indices)\n",
    "    dataframe_list.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_down = ak.stock_cy_a_spot_em()\n",
    "# 创建股票数据列表\n",
    "stock_down = ak.stock_cy_a_spot_em() # 创业板实时数据\n",
    "stock_list = stock_down[~stock_down['名称'].str.contains(\"退|ST\") & (stock_down['流通市值'] <= 1e11) & (stock_down['总市值'] >= 45e8)] # 去除退市和ST股票\n",
    "file_name_cy = 'Stock_list_cy.csv'# 保存数据，编码格式为utf-8\n",
    "stock_list.to_csv(args.root_path + file_name_cy,index=False,encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 读取股票列表\n",
    "stock_list = pd.read_csv(args.root_path + file_name_cy) # 读取股票列表\n",
    "# 将股票代码的数字转换为字符串列表\n",
    "stock_list = [str(code) for code in stock_list['代码'].tolist()]\n",
    "\n",
    "# stock_list = ['600028'] # 自定义股票列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "获取数据时间为： 20151201 - 20231220\n",
      "原始数据形状： (1961, 9)\n",
      "添加数据以后形状： (1961, 33)\n",
      "删除指定行、列后数据形状:  (1858, 32)\n",
      "股票代码 600028 的数据保存完毕，形状: (1858, 32)\n",
      "处理进度: 1/1 (100.00%)\n",
      "单独数据保存完毕。\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "all_data_raw = pd.DataFrame()  # 初始化一个空的 DataFrame 用于存储原始数据\n",
    "all_data_scaled = pd.DataFrame()  # 初始化一个空的 DataFrame 用于存储标准化后的数据\n",
    "processed_count = 0  # 初始化计数器\n",
    "total_count = len(stock_list)  # 获取总股票数量\n",
    "scaler = StandardScaler()\n",
    "\n",
    "for i in stock_list[:]:\n",
    "    NUM = i\n",
    "    try:\n",
    "        # 下载原始数据\n",
    "        raw_data = download_data(NUM, args)\n",
    "        # 更新已处理股票数量计数器\n",
    "        processed_count += 1\n",
    "        # 检查数据长度，如果小于300则跳过此次循环\n",
    "        if raw_data.shape[0] < 280:\n",
    "            print(f\"股票代码 {NUM} 的数据长度小于300,跳过此次循环。\")\n",
    "            continue\n",
    "        \n",
    "        # 拼接数据，添加各种参数\n",
    "        ad_data = add_data(raw_data, args)\n",
    "        # 添加预测标签\n",
    "        # ad_data = add_label(ad_data, args)\n",
    "        # 删除无效数据\n",
    "        final_data_raw = sub_data(ad_data.copy(), args)\n",
    "        final_data_scaled = final_data_raw.copy()\n",
    "        \n",
    "        # 未处理数据的前20行替换为0\n",
    "        if args.data_addzero and not all_data_raw.empty:\n",
    "            final_data_raw = add_zeros_to_data(final_data_raw, num_rows=args.data_addzero)\n",
    "\n",
    "        # 标准化处理\n",
    "        non_time_columns = final_data_scaled.columns[1:-1]  # 假设时间列是第一列\n",
    "        final_data_scaled[non_time_columns] = scaler.fit_transform(final_data_scaled[non_time_columns])\n",
    "\n",
    "        # 标准化处理的数据的前20行替换为0\n",
    "        if args.data_addzero and not all_data_scaled.empty:\n",
    "            final_data_scaled = add_zeros_to_data(final_data_scaled, num_rows=args.data_addzero)\n",
    "        \n",
    "        # 如果 all 为 False, 则为每个股票单独保存数据\n",
    "        if not args.all:\n",
    "            file_name_raw_individual = f\"raw_stock_cy_{NUM}.csv\"\n",
    "            file_name_scaled_individual = f\"scaled_stock_cy_{NUM}.csv\"\n",
    "            final_data_raw.to_csv(args.root_path + file_name_raw_individual, index=False)\n",
    "            final_data_scaled.to_csv(args.root_path + file_name_scaled_individual, index=False)\n",
    "            print(f\"股票代码 {NUM} 的数据保存完毕，形状: {final_data_raw.shape}\")\n",
    "        else:\n",
    "            # 拼接未处理的数据\n",
    "            all_data_raw = pd.concat([all_data_raw, final_data_raw], ignore_index=True)\n",
    "            # 拼接经过标准化的数据\n",
    "            all_data_scaled = pd.concat([all_data_scaled, final_data_scaled], ignore_index=True)\n",
    "            print(f\"当前all_data的形状: {all_data_scaled.shape}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"处理股票代码 {NUM} 时出现错误: {e}\")\n",
    "        continue\n",
    "\n",
    "    # 计算并打印处理进度\n",
    "    progress = processed_count / total_count\n",
    "    print(f\"处理进度: {processed_count}/{total_count} ({progress:.2%})\")\n",
    "\n",
    "# 如果 all 为 True, 则保存合并后的数据\n",
    "if args.all:\n",
    "    time = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "    file_name_raw = \"stock_cy_all_raw.csv\"\n",
    "    file_name_scaled = \"stock_cy_all_scaled.csv\"\n",
    "    all_data_raw.to_csv(args.root_path + time + file_name_raw, index=False)\n",
    "    all_data_scaled.to_csv(args.root_path + time + file_name_scaled, index=False)\n",
    "    print(\"合并数据保存完毕。\")\n",
    "else:\n",
    "    print(\"单独数据保存完毕。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
